{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PaintingClassification_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyfroDgLe3fi"
      },
      "source": [
        "# **Classification of Paintings**\n",
        "*   Authors: Nils Berns, John Kimani\n",
        "*   Version date: Feb 2, 2021\n",
        "*   Project name: Classification of Paintings\n",
        "*   Done as the final project of the course \"Deep Learning\" at opencampus.sh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-yJkRJwAXQs"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import shutil\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from random import uniform, randint\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zo3H-90ft5k"
      },
      "source": [
        "# Set up General Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrjb1ndlEF0o"
      },
      "source": [
        "image_size = 128 # size of the images used in training (squared images)\n",
        "min_paintings = 200 # minimum number of paintings an artist has to have to be included\n",
        "re_size = 300 # length of squared image in pixels the fake paintings are resized to before they are copied to data set folders, should not exceed 512\n",
        "download_BestArtworks = False # set to True if you want to download the data, makes sense on the first run\n",
        "download_MonetStylised = False # set to True if you want to download the data, makes sense on the first run\n",
        "use_fakes = False # classify only true paintings or include the artificials (fake) as well\n",
        "use_default_model = False # set to True if you want to use our best model architecture\n",
        "\n",
        "# best results were obtained with\n",
        "# data augmentation\n",
        "# image_size = 480\n",
        "# min_paintings = 300\n",
        "# use_default_model = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuOB0DyIf8Dv"
      },
      "source": [
        "# Download the Datasets\n",
        "From Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so1o7AwrJl9g"
      },
      "source": [
        "# Download the datasets from Kaggle \n",
        "# follow these instruction: https://medium.com/analytics-vidhya/how-to-fetch-kaggle-datasets-into-google-colab-ea682569851a\n",
        "\n",
        "try:\n",
        "  os.mkdir('/content/gdrive/My Drive/Kaggle')\n",
        "except OSError:\n",
        "  pass\n",
        "# changing the working directory\n",
        "%cd /content/gdrive/My Drive/Kaggle/\n",
        "\n",
        "if download_BestArtworks == True:\n",
        "  # /content/gdrive/My Drive/Kaggle is the path where kaggle.json is present in the Google Drive\n",
        "  os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/My Drive/Kaggle'\n",
        "  # dowload and extract the data from Kaggle, once done its found in myDrive\n",
        "  !kaggle datasets download -d ikarus777/best-artworks-of-all-time\n",
        "  # unzipping the zip files and deleting the zip files\n",
        "  !unzip \\*.zip  && rm *.zip\n",
        "\n",
        "if download_MonetStylised == True:\n",
        "  # /content/gdrive/My Drive/Kaggle is the path where kaggle.json is present in the Google Drive\n",
        "  os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/My Drive/Kaggle'\n",
        "  # dowload and extract the data from Kaggle, once done its found in myDrive\n",
        "  !kaggle datasets download -d shcsteven/paired-landscape-and-monetstylised-image\n",
        "  # unzipping the zip files and deleting the zip files\n",
        "  !unzip \\*.zip  && rm *.zip\n",
        "  destination = '/content/gdrive/My Drive/Kaggle/Monet_stylised/'\n",
        "  try:\n",
        "    os.mkdir(destination)\n",
        "  except OSError:\n",
        "    pass\n",
        "  for source in ['/content/gdrive/My Drive/Kaggle/monet_style_dataset/monet_style_dataset_A/stylized_A/', '/content/gdrive/My Drive/Kaggle/monet_style_dataset/monet_style_dataset_B/stylized_B/']:\n",
        "    for fname in os.listdir(source):\n",
        "      shutil.copyfile(source + fname, destination + fname)\n",
        "  shutil.rmtree('/content/gdrive/My Drive/Kaggle/monet_style_dataset/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vJdHHj8gTWy"
      },
      "source": [
        "From https://thisartworkdoesnotexist.com/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P48bbwhD6CA"
      },
      "source": [
        "# Download artificial paintings from https://thisartworkdoesnotexist.com/\n",
        "\n",
        "destination = '/content/gdrive/My Drive/Kaggle/thisartworkdoesnotexist/'\n",
        "try:\n",
        "  os.mkdir(destination)\n",
        "except OSError:\n",
        "    pass\n",
        "nof = len(os.listdir(destination)) # number of images in the directory\n",
        "desired_nof = 1000 # desired number of images from this source\n",
        "if nof < desired_nof:\n",
        "    for ii in range(nof, desired_nof):\n",
        "        url = 'https://thisartworkdoesnotexist.com/'\n",
        "        r = requests.get(url)\n",
        "        fname = f'thisdoesnotexist_fake_painting_{ii}.jpg'\n",
        "        with open(destination + fname, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "        time.sleep(1) # time-out of ~1s required for requesting a novel image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbmO_PKIgZxB"
      },
      "source": [
        "From https://boredhumans.b-cdn.net/art/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgxERF04J90D"
      },
      "source": [
        "# Download artificial paintings from https://boredhumans.b-cdn.net/art/\n",
        "\n",
        "destination = '/content/gdrive/My Drive/Kaggle/boredhumans_stylegan2/'\n",
        "random.seed(100)\n",
        "try:\n",
        "    os.mkdir(destination)\n",
        "except OSError:\n",
        "    pass\n",
        "filenumbers = os.listdir(destination)\n",
        "nof = len(filenumbers) # number of images in the directory\n",
        "desired_nof = 1000 # desired number of images from this source\n",
        "for jj in range(nof):\n",
        "    filenumbers[jj] = int(filenumbers[jj][20:-4])\n",
        "if nof < desired_nof:\n",
        "    randomlist = []\n",
        "    for ii in range(nof, desired_nof):\n",
        "        n = random.randint(1,5001)\n",
        "        while (n in randomlist) or (n in filenumbers):\n",
        "            n = random.randint(1,5001)\n",
        "        randomlist.append(n)\n",
        "        url = f'https://boredhumans.b-cdn.net/art/{n}.jpg'\n",
        "        r = requests.get(url)\n",
        "        fname = f'bored_fake_painting_{n}.jpg'\n",
        "        with open(destination + fname, 'wb') as f:\n",
        "            f.write(r.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAmNHrtUgdtk"
      },
      "source": [
        "# Pre-processing the Data\n",
        "Read data about the artists from csv-file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnP1Gg0gOaFA"
      },
      "source": [
        "def read_artists():\n",
        "  artists_names = []\n",
        "  num_of_paintings = []\n",
        "  genres = []\n",
        "  with open('artists.csv') as artists_file:\n",
        "    csv_reader = csv.reader(artists_file, delimiter=',') # delimiter is comma\n",
        "    next(csv_reader) # skip header\n",
        "    for row in csv_reader:\n",
        "      if row[1] == 'Albrecht DÃ¼rer':\n",
        "        temp_name = 'Albrecht Durer'\n",
        "      elif row[1] == 'Vasiliy Kandinskiy':\n",
        "        temp_name = 'Wassily Kandinsky'\n",
        "      else:  \n",
        "        temp_name = row[1]\n",
        "      genres.append(row[3])\n",
        "      num_of_paintings.append(int(row[-1]))\n",
        "      artists_names.append(temp_name)\n",
        "  return artists_names, num_of_paintings, genres\n",
        "\n",
        "artists_names_csv, num_of_paintings_csv, genre_csv = read_artists()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GrL_1LNg4cp"
      },
      "source": [
        "The directory with respect to classification problem is chosen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwYqIF3M902k"
      },
      "source": [
        "if use_fakes == True:\n",
        "  data_directory = '/content/gdrive/My Drive/Kaggle/painting_classification/fake_detector/'\n",
        "  print('Include fake paintings!')\n",
        "else:\n",
        "  data_directory = '/content/gdrive/My Drive/Kaggle/painting_classification/'\n",
        "  print('Only true paintings data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIbu_9gShRXa"
      },
      "source": [
        "Truncate the list of artists with respect to the desired minimum number of paintings per artist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDuQcfMrWQbF"
      },
      "source": [
        "# exclude artists with less than min_paintings paintings to improve validation accuracy\n",
        "artists_names = []\n",
        "num_of_paintings = []\n",
        "genre = []\n",
        "for ii in range(len(artists_names_csv)):\n",
        "  if num_of_paintings_csv[ii] > min_paintings:\n",
        "    artists_names.append(artists_names_csv[ii])\n",
        "    num_of_paintings.append(num_of_paintings_csv[ii])\n",
        "    genre.append(genre_csv[ii])\n",
        "\n",
        "if use_fakes == True:\n",
        "  num_of_classes = len(artists_names) + 1\n",
        "else:\n",
        "  num_of_classes = len(artists_names)\n",
        "\n",
        "print(f'{len(artists_names)} out fo 50 artists have more than {min_paintings} painting(s).')\n",
        "\n",
        "for ii in range(len(genre)):\n",
        "  print(f'{artists_names[ii]}: {genre[ii]}')\n",
        "# 4 out of 11 are from the Renaissance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RzU6SDhhou6"
      },
      "source": [
        "Create required directories for sorting the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWV8dfnEQ2pY"
      },
      "source": [
        "def create_directory(DIRs):\n",
        "  if type(DIRs) == list:\n",
        "    pass\n",
        "  elif type(DIRs) == str:\n",
        "    DIRs = [DIRs]\n",
        "  else:\n",
        "    print('No directory created. Input type neither list nor string.')\n",
        "    return\n",
        "  for DIR in DIRs:\n",
        "    try:\n",
        "      os.mkdir(DIR)\n",
        "    except OSError:\n",
        "      pass\n",
        "  return\n",
        "\n",
        "make_directories = ['/content/gdrive/My Drive/Kaggle/images/',\n",
        "                    '/content/gdrive/My Drive/Kaggle/images/images/'\n",
        "                   ]\n",
        "\n",
        "directories = ['/content/gdrive/My Drive/Kaggle/painting_classification/',\n",
        "               '/content/gdrive/My Drive/Kaggle/painting_classification/fake_detector/'\n",
        "               ]\n",
        "\n",
        "for directory in directories:\n",
        "  make_directories.append(directory)\n",
        "  names = []\n",
        "  if directory[-14:] == 'fake_detector/':\n",
        "    names = artists_names.copy()\n",
        "    names.append('Fake')\n",
        "  else:\n",
        "    names = artists_names.copy()\n",
        "  for folder in ['training/', 'development/', 'testing/']:\n",
        "    make_directories.append(directory + folder)\n",
        "    for artist in names:\n",
        "        make_directories.append(directory + folder + artist + '/')\n",
        "\n",
        "create_directory(make_directories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI0FzcM2hyER"
      },
      "source": [
        "Define the size of the training set etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMNgEadyQ813"
      },
      "source": [
        "random.seed(1)\n",
        "train_size = 0.8\n",
        "dev_size = (1 - train_size)*0.5\n",
        "test_size = (1 - train_size)*0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1wJ0j0dh8fP"
      },
      "source": [
        "Move the images of the desired artists to the training, development, testing folders. If a class of fraudulent paintings is added, copy the images to the folders within the fake_detector directory too. Also copy resized versions of the images from the fake class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL6Poqe3aawp"
      },
      "source": [
        "source = '/content/gdrive/My Drive/Kaggle/images/images/'\n",
        "destination = '/content/gdrive/My Drive/Kaggle/painting_classification/'\n",
        "\n",
        "artists_folder = os.listdir(source)\n",
        "index = []\n",
        "for ii in range(len(artists_folder)):\n",
        "  if artists_folder[ii][:8] == 'Albrecht':\n",
        "    index.append(ii)\n",
        "if len(index) > 1:\n",
        "  for ii in index[1:]:\n",
        "    shutil.rmtree(source + artists_folder[ii] + '/')\n",
        "\n",
        "artists_folder = os.listdir(source)\n",
        "for ii in range(len(artists_folder)):\n",
        "  if artists_folder[ii][:8] == 'Albrecht':\n",
        "    artist = 'Albrecht Durer'\n",
        "  elif artists_folder[ii] == 'Vasiliy_Kandinskiy':\n",
        "    artist = 'Wassily Kandinsky'\n",
        "  else:\n",
        "    artist = artists_folder[ii]\n",
        "    artist = artist.replace('_', ' ')\n",
        "  if artist in artists_names:\n",
        "    print(f'{artist}: copy data to generator folder')\n",
        "    source_artist = source + artists_folder[ii] + '/'\n",
        "    random.seed(ii)\n",
        "    images = os.listdir(source_artist)\n",
        "    test_length = int(test_size*len(images))\n",
        "    dev_length = int(dev_size*len(images))\n",
        "    train_length = len(images)- test_length - dev_length\n",
        "    shuffled_set = random.sample(images, len(images))\n",
        "    train_set = shuffled_set[:train_length]\n",
        "    dev_set = shuffled_set[train_length:train_length+dev_length]\n",
        "    test_set = shuffled_set[train_length+dev_length:]\n",
        "    sets = {\n",
        "        'training/': train_set,\n",
        "        'development/': dev_set,\n",
        "        'testing/': test_set\n",
        "    }\n",
        "    for folder in ['training/', 'development/', 'testing/']:\n",
        "      for fname in sets[folder]:\n",
        "        destination_set = destination + folder + artist + '/'\n",
        "        shutil.copyfile(source_artist + fname, destination_set + fname)\n",
        "    shutil.rmtree(source_artist)\n",
        "\n",
        "for artist in os.listdir(destination + 'training/'):\n",
        "  if artist not in artists_names:\n",
        "    print(f'{artist}: copy data to source folder')\n",
        "    create_directory(source + artist + '/')\n",
        "    for folder in ['training/', 'development/', 'testing/']:\n",
        "      for fname in os.listdir(destination + folder + artist + '/'):\n",
        "        shutil.copyfile(destination + folder + artist + '/' + fname, source + artist + '/' + fname)\n",
        "      shutil.rmtree(destination + folder + artist + '/')\n",
        "\n",
        "if use_fakes == True:\n",
        "  source = '/content/gdrive/My Drive/Kaggle/painting_classification/'\n",
        "  destination = '/content/gdrive/My Drive/Kaggle/painting_classification/fake_detector/'\n",
        "  for folder in ['training/', 'development/', 'testing/']:\n",
        "    for artist in artists_names:\n",
        "      if len(os.listdir(destination + folder + artist + '/')) == 0:\n",
        "        if folder == 'training/':\n",
        "          print(f'{artist}: copy data to fake_detector generator folder')\n",
        "        for fname in os.listdir(source + folder + artist + '/'):\n",
        "          img = Image.open(source + folder + artist + '/' + fname)\n",
        "          img = img.resize((re_size, re_size))\n",
        "          img.save(destination + folder + artist + '/' + fname)\n",
        "  for artist in os.listdir(destination + 'training/'):\n",
        "    if (artist not in artists_names) and (artist != 'Fake'):\n",
        "      for folder in ['training/', 'development/', 'testing/']:\n",
        "        shutil.rmtree(destination + folder + artist + '/')\n",
        "  if len(os.listdir(destination + 'training/Fake/')) == 0:\n",
        "    random.seed(75)\n",
        "    sources_fake = ['/content/gdrive/My Drive/Kaggle/boredhumans_stylegan2/',\n",
        "                    '/content/gdrive/My Drive/Kaggle/Monet_stylised/',\n",
        "                    '/content/gdrive/My Drive/Kaggle/thisartworkdoesnotexist/'\n",
        "                   ]\n",
        "    for source_fake in sources_fake:\n",
        "      print(f'{source_fake}: copy data to fake_detector generator fake folder')\n",
        "      images = os.listdir(source_fake)\n",
        "      test_length = int(test_size*len(images))\n",
        "      dev_length = int(dev_size*len(images))\n",
        "      train_length = len(images)- test_length - dev_length\n",
        "      shuffled_set = random.sample(images, len(images))\n",
        "      train_set = shuffled_set[:train_length]\n",
        "      dev_set = shuffled_set[train_length:train_length+dev_length]\n",
        "      test_set = shuffled_set[train_length+dev_length:]\n",
        "      sets = {\n",
        "          'training/': train_set,\n",
        "          'development/': dev_set,\n",
        "          'testing/': test_set \n",
        "      }\n",
        "      for folder in ['training/', 'development/', 'testing/']:\n",
        "        for fname in sets[folder]:\n",
        "          img = Image.open(source_fake + fname)\n",
        "          img = img.resize((re_size, re_size))\n",
        "          img.save(destination + folder + 'Fake/' + fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8t-Zf-OjBh-"
      },
      "source": [
        "Count the number of images used for the classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyu2TWLXbkpu"
      },
      "source": [
        "for folder in ['training', 'development', 'testing']:\n",
        "  counter = 0\n",
        "  for artist in os.listdir(data_directory + folder + '/'):\n",
        "    counter = counter + len(os.listdir(data_directory + folder + '/' + artist + '/'))\n",
        "    if folder == 'training':\n",
        "      print(artist, len(os.listdir(data_directory + folder + '/' + artist + '/')))\n",
        "    if len(os.listdir(data_directory + folder + '/' + artist + '/')) == 0:\n",
        "      print(f'{folder} - {artist} is empty')\n",
        "  print(f'Paintings in {folder}-set: {counter}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMqVvmbSjpsU"
      },
      "source": [
        "Function used to generate the training and validation sets from directories. Data augmentation is included."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNKASm0LYs9z"
      },
      "source": [
        "def create_generators(batch_size=128, image_size=128, DIR=data_directory):\n",
        "  TRAINING_DIR = DIR + 'training/'\n",
        "  train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "        )\n",
        "  train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      class_mode='categorical',\n",
        "                                                      shuffle=True,\n",
        "                                                      target_size=(image_size, image_size))\n",
        "\n",
        "  VALIDATION_DIR = DIR + 'development/'\n",
        "  validation_datagen = ImageDataGenerator(rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "        )\n",
        "  validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                                batch_size=batch_size,\n",
        "                                                                class_mode='categorical',\n",
        "                                                                shuffle=True,\n",
        "                                                                target_size=(image_size, image_size))\n",
        "  return train_generator, validation_generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJeloTWjj50K"
      },
      "source": [
        "# Model Architecture\n",
        "Function that creates a model based on the input arguments or default values. A default model architecture is included which gave us the best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksbC_jXqe3Ky"
      },
      "source": [
        "def create_model(default_model=use_default_model, nDense=2, minNeurons=256, nConLay=4, minCons=32, con_size=3, pool_size=2, batch_norm=False, drop_rate=0.0, nOutputs=num_of_classes, image_size=image_size):\n",
        "  # -> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC -> (Ioffe and Szegedy 2015)\n",
        "  # However Szegedy is known to now prefer applying the batch normalisation after the activation.\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Input(shape=(image_size, image_size, 3)))\n",
        "  if default_model == False:\n",
        "    m = 0\n",
        "    for ii in range(nConLay):\n",
        "      if ii % 2 == 0 and ii > 0:\n",
        "        m += 1\n",
        "      model.add(tf.keras.layers.Conv2D(minCons*2**m, (con_size, con_size), activation='relu', padding='same')) #, use_bias=False, kernel_regularizer=l2(1e-4)),\n",
        "      if batch_norm == True:\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "      model.add(tf.keras.layers.MaxPooling2D(pool_size, pool_size))\n",
        "      if ii+1 % 2 == 0:\n",
        "        model.add(tf.keras.layers.Dropout(drop_rate))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    for ii in range(nDense):\n",
        "      model.add(tf.keras.layers.Dense(2**(nDense-1-ii)*minNeurons, activation='relu'),)\n",
        "      # model.add(tf.keras.layers.Dense(\n",
        "      #                 2**(nDense-1-ii)*minNeurons,\n",
        "      #                 kernel_initializer='ones',\n",
        "      #                 kernel_regularizer=tf.keras.regularizers.L1(0.01),\n",
        "      #                 activity_regularizer=tf.keras.regularizers.L2(0.01),\n",
        "      #                 activation='relu'))\n",
        "      # regularisation works the same with Conv2D layers\n",
        "  else: # use our default architecture\n",
        "    model.add(tf.keras.layers.Conv2D(16, (3,3), padding = \"same\", strides = 2, activation='relu'))\n",
        "    model.add(tf.keras.layers.Conv2D(16, (3,3), padding = \"same\", strides = 2, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(3,2))\n",
        "    model.add(tf.keras.layers.Conv2D(32, (3,3), padding = \"same\", activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(3,2))\n",
        "    model.add(tf.keras.layers.Conv2D(48, (3,3), padding = \"same\", activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(3,2))\n",
        "    model.add(tf.keras.layers.Conv2D(64, (3,3), padding = \"same\", activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(3,2))\n",
        "    model.add(tf.keras.layers.Conv2D(80, (3,3), padding = \"same\", activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPooling2D(3,2)) # even with an image size of 512 by 512 this pooling layer leads to an error due to size\n",
        "    # model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(nOutputs, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=tf.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od1fpzhylNne"
      },
      "source": [
        "# Random Grid Search for Hyperparameters\n",
        "Define the parameters of the random grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsUBlugFfcF6"
      },
      "source": [
        "random.seed(85)\n",
        "len_arrays = 10 # not very important\n",
        "batch_size_r = [randint(64, 256) for i in range(len_arrays)]\n",
        "epochs = [randint(20, 100) for i in range(len_arrays)] # use comma for more values ex. [10, 50, 100]\n",
        "minNeurons = [randint(128, 512) for i in range(len_arrays)]\n",
        "nDense = [randint(1, 4) for i in range(len_arrays)]\n",
        "nConLay = [randint(2, 6) for i in range(len_arrays)]\n",
        "minCons = [randint(16, 64) for i in range(len_arrays)]\n",
        "con_size= [randint(3, 5) for i in range(len_arrays)]\n",
        "pool_size= [randint(2, 4) for i in range(len_arrays)]\n",
        "# batch_norm=False\n",
        "drop_rate = [uniform(0., 0.3) for i in range(len_arrays)]\n",
        "# this is important\n",
        "num_of_parameters_that_we_want_to_use = 9\n",
        "param_random_grid = dict(batch_size=batch_size_r,\n",
        "                         epochs=epochs,\n",
        "                         minNeurons=minNeurons,\n",
        "                         nDense=nDense,\n",
        "                         nConLay=nConLay,\n",
        "                         minCons=minCons,\n",
        "                         con_size=con_size,\n",
        "                         pool_size=pool_size,\n",
        "                         drop_rate=drop_rate)\n",
        "# what does the dictionary look like?\n",
        "print(param_random_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNKwi6t-mm6P"
      },
      "source": [
        "Run the random search for hyperparameters. The used way does not offer parallelised search iterations, unfortunately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFJ94OXpmCq6"
      },
      "source": [
        "history = []\n",
        "for ii in range(len_arrays):\n",
        "  train_generator, validation_generator = create_generators(batch_size=batch_size_r[ii], image_size=image_size)\n",
        "  random_model = create_model(default_model=use_default_model, nDense=nDense[ii], minNeurons=minNeurons[ii], nConLay=nConLay[ii], minCons=minCons[ii], con_size=con_size[ii], pool_size=2, batch_norm=False, drop_rate=drop_rate[ii], nOutputs=num_of_classes, image_size=image_size)\n",
        "  random_model.summary()\n",
        "\n",
        "  history.append(random_model.fit(train_generator,\n",
        "                           epochs=epochs[ii],\n",
        "                           verbose=1,\n",
        "                           validation_data=validation_generator)\n",
        "                )\n",
        "  \n",
        "  acc = history[ii].history['accuracy']\n",
        "  val_acc = history[ii].history['val_accuracy']\n",
        "  loss = history[ii].history['loss']\n",
        "  val_loss = history[ii].history['val_loss']\n",
        "\n",
        "  epochs_plot = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs_plot, acc, 'r', label='Training accuracy')\n",
        "  plt.plot(epochs_plot, val_acc, 'b', label='Validation accuracy')\n",
        "  plt.title(f'Training and validation accuracy of model {ii}')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs_plot, loss, 'r', label='Training Loss')\n",
        "  plt.plot(epochs_plot, val_loss, 'b', label='Validation Loss')\n",
        "  plt.title(f'Training and validation loss of model {ii}')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcHJc_LonILH"
      },
      "source": [
        "# Training the Model the Conservative Way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37VHZtK1d7R6"
      },
      "source": [
        "train_generator, validation_generator = create_generators(batch_size=128, image_size=image_size)\n",
        "model = create_model() # the model architecture is defined through arguments here\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "                           epochs=80,\n",
        "                           verbose=1,\n",
        "                           validation_data=validation_generator)\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_plot = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_plot, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs_plot, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_plot, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs_plot, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FIaoGd-DKWx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}