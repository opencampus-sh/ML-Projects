---
title: "Estimation of Neural Net"
output: html_notebook
---

### Installation von Python und TensorFlow (nur einmalig nötig)
```{r}
install.packages("reticulate")
library(reticulate)

# Installation von miniconda (falls nicht vorhanden)
install_miniconda(update=TRUE)

# Anlegen einer speziellen Python Umgebung
conda_create("r-reticulate")

# Installieren der Pakete in der angelegten Umgebung
conda_install("r-reticulate", "pandas")
conda_install("r-reticulate", "numpy")
conda_install("r-reticulate", "tensorflow")
conda_install("r-reticulate", "h5py")
 
# Verwenden der speziellen Python Umgebung die zuvor erstellt wurde
use_condaenv("r-reticulate")



```


### Run script for data preparation
```{r}
#source("NeuralNetDataPreparation.R")

```


### Importing relevant packages
```{r}
library(reticulate)
library(ggplot2)
library(Metrics)
library(tidyverse)
library(lubridate)
library(forecast)

```

### Split the base dataset into train_actuals containing the original training data

```{r}
train_actuals <- bike_data %>%
  select(interval, bikes_rented,bikes_returned) %>%
  slice(train_ind)
test_actuals <- bike_data %>%
  select(interval, bikes_rented,bikes_returned) %>%
  slice(-train_ind)
data_train<-train_actuals
data_test<-test_actuals
```


### Definition of neural net
```{python}
# import relevant Python libraries
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.regularizers import l2,l1

#Initializer for ReLU
initializer= tf.keras.initializers.VarianceScaling(
    scale=2.0, mode="fan_in", distribution="untruncated_normal", seed=31
)

regularizer=tf.keras.regularizers.l2(0.01)
activation_function="relu"
# Definition of form and depth of neural net (Deep Neural Net)
model_rented = tf.keras.Sequential([
  keras.layers.Dense(20, kernel_initializer=initializer,kernel_regularizer=l2(0.1), activation='relu', input_shape=[len(r.train_dataset.keys())]),
  keras.layers.Dense(10, kernel_initializer=initializer,kernel_regularizer=l2(0.1), activation='relu'),
  keras.layers.Dense(10, kernel_initializer=initializer,kernel_regularizer=l2(0.1), activation='relu'),
  keras.layers.Dense(10, kernel_initializer=initializer,kernel_regularizer=l2(0.1), activation='relu'),
  keras.layers.Dense(1)
])
# Definition der Kosten-(Loss-)Funktion und der Optimierungsfunktion mit seinen Hyperparametern
model_rented.compile(loss="mse", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005,beta_1=0.9, beta_2=0.999))

# Ausgabe einer Zusammenfassung zur Form des Modells, das geschaetzt wird (nicht notwendig)
model_rented.summary()

model_returned = tf.keras.Sequential([
  keras.layers.Dense(20, kernel_initializer=initializer,kernel_regularizer=l2(0.1), activation='relu', input_shape=[len(r.train_dataset.keys())]),
  keras.layers.Dense(10, kernel_initializer=initializer,kernel_regularizer=l2(0.1), activation='relu'),
  keras.layers.Dense(10, kernel_initializer=initializer,kernel_regularizer=l2(0.1), activation='relu'),
  keras.layers.Dense(10, kernel_initializer=initializer,kernel_regularizer=l2(0.1), activation='relu'),
  keras.layers.Dense(1)
])

# Definition der Kosten-(Loss-)Funktion und der Optimierungsfunktion mit seinen Hyperparametern
model_returned.compile(loss="mse", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005,beta_1=0.9, beta_2=0.999))

# Ausgabe einer Zusammenfassung zur Form des Modells, das geschaetzt wird (nicht notwendig)
model_returned.summary()


nbr_epochs=15000
#r.train_labels["UmsatzWG1"]

```


### Schätzung des neuronalen Netzes
```{python}
# Schaetzung des Modells
history_rented = model_rented.fit(r.train_dataset, r.train_labels["bikes_rented"], epochs=nbr_epochs, batch_size=32,
                    validation_data = (r.test_dataset, r.test_labels["bikes_rented"]), verbose=0)

# Ggf. Speichern des geschaetzten Modells
model_rented.save("python_model_rentedHBF.h5")

```



### Auswertung der Modelloptimierung
```{r}
# Grafische Ausgabe der Modelloptimierung

# create data
data <- data.frame(val_loss = unlist(py$history_rented$history$val_loss),
                  loss = unlist(py$history_rented$history$loss))

# Plot
ggplot(data[-(1:1000),]) +
  geom_line( aes(x=1:length(val_loss), y=val_loss, colour = "Validation Loss" )) +
  geom_line( aes(x=1:length(loss), y=loss, colour = "Training Loss" )) +
  scale_colour_manual( values = c("Training Loss"="blue", "Validation Loss"="red") ) +
  labs(title="Loss Function Values During Optimization") +
  xlab("Iteration Number") +
  ylab("Loss") 


```


### Load a trained model ###
```{python}
#model_rented = keras.models.load_model("python_model_rentedHBFColab3.h5")

```


### Prediction Results ###

Only One Label UmsatzWG1
```{r}
# Prediction of the normailzed data for training and test
train_predictions_norm <- py$model_rented$predict(train_dataset)
test_predictions_norm <- py$model_rented$predict(test_dataset)

# Normalized values to real values
train_predictions<-as_tibble(train_predictions_norm)
test_predictions<-as_tibble(test_predictions_norm)

train_predictions<- train_predictions %>% 
  mutate(bikes_rented=(train_predictions$V1 * norm_values_list$sd[1] ) + norm_values_list$mean[1]) %>%
  select(bikes_rented)
  
test_predictions<- test_predictions %>% 
  mutate(bikes_rented=(test_predictions$V1 * norm_values_list$sd[1] ) + norm_values_list$mean[1]) %>%
  select(bikes_rented)


# Some measures
cat(paste0("MAPE on the Training Data:\t", format(mape(train_actuals$bikes_rented, train_predictions$bikes_rented)*100, digits=3, nsmall=2)))
cat(paste0("\nMAPE on the Validation Data:\t", format(mape(test_actuals$bikes_rented, test_predictions$bikes_rented)*100, digits=3, nsmall=2)))

cat(paste0("\nMAE on the Training Data:\t", format(mae(train_actuals$bikes_rented, train_predictions$bikes_rented), digits=3, nsmall=2)))
cat(paste0("\nMAE on the Validation Data:\t", format(mae(test_actuals$bikes_rented, test_predictions$bikes_rented), digits=3, nsmall=2)))

cat(paste0("\nMSE on the Training Data:\t", format(mse(train_actuals$bikes_rented, train_predictions$bikes_rented), digits=3, nsmall=2)))
cat(paste0("\nMSE on the Validation Data:\t", format(mse(test_actuals$bikes_rented, test_predictions$bikes_rented), digits=3, nsmall=2)))

```

### Bikes rented
```{r}

## Graphical comparison opf predicted and real values

# Collect data for plots
#data_train <- data.frame(prediction = train_predictions, actual = train_actuals)
#data_test <- data.frame(prediction = test_predictions, actual = test_actuals)
data_train <- data_train %>%
  mutate(pred_bikes_rented=train_predictions$bikes_rented)
data_test <- data_test %>%
  mutate(pred_bikes_rented=test_predictions$bikes_rented)

# Training data
ggplot(data_train[-(1:150),]) +
  geom_line( aes(x=1:length(pred_bikes_rented), y=pred_bikes_rented, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(bikes_rented), y=bikes_rented, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Training Data") +
  xlab("time interval") +
  ylab("bikes rented") 

# Test data
ggplot(data_test) +
  geom_line( aes(x=1:length(pred_bikes_rented), y=pred_bikes_rented, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(bikes_rented), y=bikes_rented, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Test Data") +
  xlab("time interval") +
  ylab("bikes rented") 
```

### Fitting the neural net
```{python}

history_returned = model_returned.fit(r.train_dataset, r.train_labels["bikes_returned"], epochs=nbr_epochs, batch_size=32,
                    validation_data = (r.test_dataset, r.test_labels["bikes_returned"]), verbose=0)

# Ggf. Speichern des geschaetzten Modells
model_returned.save("python_model_returnedHBF.h5")

```


### Auswertung der Modelloptimierung
```{r}
# Graphical view on model optimization

# create data
data <- data.frame(val_loss = unlist(py$history_returned$history$val_loss),
                  loss = unlist(py$history_returned$history$loss))

# Plot
ggplot(data[-(1:1500),]) +
  geom_line( aes(x=1:length(val_loss), y=val_loss, colour = "Validation Loss" )) +
  geom_line( aes(x=1:length(loss), y=loss, colour = "Training Loss" )) +
  scale_colour_manual( values = c("Training Loss"="blue", "Validation Loss"="red") ) +
  labs(title="Loss Function Values During Optimization") +
  xlab("Iteration Number") +
  ylab("Loss") 


```


### (Ggf.) Laden eines gespeicherten Neuronalen Netzes ###
```{python}
#model_returned = keras.models.load_model("python_model_returnedHBFColab3.h5")

```


### Prediction results for bikes returned ###

```{r}
# Prediction of normaized values
train_predictions_norm <- py$model_returned$predict(train_dataset)
test_predictions_norm <- py$model_returned$predict(test_dataset)

# Normalized to real values
train_predictions<-as_tibble(train_predictions_norm)
test_predictions<-as_tibble(test_predictions_norm)

train_predictions<- train_predictions %>% 
  mutate(bikes_returned=(train_predictions$V1 * norm_values_list$sd[30] ) + norm_values_list$mean[30]) %>%
  select(bikes_returned)
  
test_predictions<- test_predictions %>% 
  mutate(bikes_returned=(test_predictions$V1 * norm_values_list$sd[30] ) + norm_values_list$mean[30]) %>%
  select(bikes_returned)

# Some measures
cat(paste0("MAPE on the Training Data:\t", format(mape(train_actuals$bikes_returned, train_predictions$bikes_returned)*100, digits=3, nsmall=2)))
cat(paste0("\nMAPE on the Validation Data:\t", format(mape(test_actuals$bikes_returned, test_predictions$bikes_returned)*100, digits=3, nsmall=2)))

cat(paste0("\nMAE on the Training Data:\t", format(mae(train_actuals$bikes_returned, train_predictions$bikes_returned), digits=3, nsmall=2)))
cat(paste0("\nMAE on the Validation Data:\t", format(mae(test_actuals$bikes_returned, test_predictions$bikes_returned), digits=3, nsmall=2)))


cat(paste0("\nMSE on the Training Data:\t", format(mse(train_actuals$bikes_returned, train_predictions$bikes_returned), digits=3, nsmall=2)))
cat(paste0("\nMSE on the Validation Data:\t", format(mse(test_actuals$bikes_returned, test_predictions$bikes_returned), digits=3, nsmall=2)))

```

### Graphics for bikes returned
```{r}

## Graphical comparison of predicted and realized values 

# Collect data
#data_train <- data.frame(prediction = train_predictions, actual = train_actuals)
#data_test <- data.frame(prediction = test_predictions, actual = test_actuals)
data_train <- data_train %>%
  mutate(pred_bikes_returned=train_predictions$bikes_returned)
data_test <- data_test %>%
  mutate(pred_bikes_returned=test_predictions$bikes_returned)

# Plot results for training data
ggplot(data_train) +
  geom_line( aes(x=1:length(pred_bikes_returned), y=pred_bikes_returned, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(bikes_returned), y=bikes_returned, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Training Data") +
  xlab("time interval") +
  ylab("bikes returned") 

# Plot results for testing data
ggplot(data_test) +
  geom_line( aes(x=1:length(pred_bikes_returned), y=pred_bikes_returned, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(bikes_returned), y=bikes_returned, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Test Data") +
  xlab("time interval") +
  ylab("bikes returned") 
```


### Data Train and Test Summary
```{r}
data_train_summary <- data_train %>%
  dplyr::summarise(
    mseWG1=mse(bikes_rented,pred_bikes_rented),
    maeWG1=mae(bikes_rented,pred_bikes_rented),
    mapeWG1=mape(bikes_rented,pred_bikes_rented),
    acc_rented =accuracy(bikes_rented, pred_bikes_rented),
    acc_returned =accuracy(bikes_returned, pred_bikes_returned),
    mseWG2=mse(bikes_returned,pred_bikes_returned),
    maeWG2=mae(bikes_returned,pred_bikes_returned),
    mapeWG2=mape(bikes_returned,pred_bikes_returned),
    n=n()
  ) 

data_train_summary <- data_train_summary %>%
  add_row(data_test %>%
  dplyr::summarise(
    mseWG1=mse(bikes_rented,pred_bikes_rented),
    maeWG1=mae(bikes_rented,pred_bikes_rented),
    mapeWG1=mape(bikes_rented,pred_bikes_rented),
    acc_rented =accuracy(bikes_rented, pred_bikes_rented),
    acc_returned =accuracy(bikes_returned, pred_bikes_returned),
    mseWG2=mse(bikes_returned,pred_bikes_returned),
    maeWG2=mae(bikes_returned,pred_bikes_returned),
    mapeWG2=mape(bikes_returned,pred_bikes_returned),
    n=n()
  ))

```
Summary statistics of model
```{r}
class(data_train_summary)
print(data_train_summary)

```



```{r}
write.csv(data_train_summary, "PythonModels/data_summary_HBF.csv")
write.csv(data_train, "PythonModels/data_train_HBF.csv")
write.csv(data_test, "PythonModels/date_test_HBF.csv")
```


Prediction for last x time intervals
```{r}
# Vorhersage für einen einzelnen Fall
nlast<-10
n<-length(data_test$interval)
cat(paste0("Prediction per :\t", data_test$interval[c(n-nlast,n)]))
cat(paste0("\nPrediction bikes rented:\t", data_test$pred_bikes_rented[c(n-nlast,n)]))
cat(paste0("\nBikes rented:\t", data_test$bikes_rented[c(n-nlast,n)]))
cat(paste0("\nPrediction bikes returned:\t", data_test$pred_bikes_returned[c(n-nlast,n)]))
cat(paste0("\nBikes returned:\t", data_test$bikes_returned[c(n-nlast,n)]))



```


