{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Climate Change.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX8JEPWma8yj"
      },
      "source": [
        "---\n",
        "## Note from the Teachers:\n",
        "We added a header with information about your project. \n",
        "This is useful since we want to keep a database of all the cool projects participants have made during the courses.\n",
        "We will save the code in our database and host on github a page to show all the projects. Therefore it is more practical to add some information in the header of each notebook.\n",
        "Thanks a lot for the understanding and for taking care of it.\n",
        "\n",
        "You can delete this cell if you want\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx2WVSK1a8yn"
      },
      "source": [
        "# Climate Change\n",
        "We try to simulate Climate Scenario Analysis using 3 different IPCC climate scenarios; RCP3PD, RCP4.5, and RCP6 and predict temperature for each scenario in the year 2100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmaZL3qza8yp"
      },
      "source": [
        "## Participants:\n",
        "Vineet Shah,\n",
        "Shilpika Sarvepalli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtI1FRzma8yq"
      },
      "source": [
        "### Course and Semester\n",
        "Deep Learning from Scratch,\n",
        "SoSe2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SN-gXoAa8yq"
      },
      "source": [
        "### License\n",
        "If you are releasing the software under some certain license, you can mention it and also include the `LICENSE.md` file in the folder\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjaZCDmPb9i3"
      },
      "source": [
        "# Climate Scenario Anaylsis\n",
        "Predicting the temperature anomaly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ0paDaAa8ys"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0dyjDC0wC7f"
      },
      "source": [
        "#Setting Up\n",
        "\n",
        "import os # import OS module to interact with Operating System\n",
        "import datetime # import module to manipulate date and time\n",
        "\n",
        "import IPython # import ipython command shell\n",
        "import IPython.display # import display tools\n",
        "\n",
        "import matplotlib as mpl # import matplot library for visualization\n",
        "import matplotlib.pyplot as plt # import library to plot timeseries data\n",
        "\n",
        "import numpy as np # import numpy library for mathematical operations\n",
        "import pandas as pd # import pandas library to process timeseries data\n",
        "import seaborn as sns # import seaborn library to process statistical graphics\n",
        "import tensorflow as tf # import tensorflow \n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow import keras # import Keras to build and summarize the model\n",
        "import plotly.express as px # import plotly.express to create graphs\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler # to normalize training data\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8,6) # setting the size of figures plotted using mpl\n",
        "mpl.rcParams['axes.grid'] = False # setting not to show gridlines on plots\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2D9qZTF3wqT"
      },
      "source": [
        "# Importing Data from drive\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['Train.csv']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZmE0uczBc5l"
      },
      "source": [
        "# Check Training Data\n",
        "\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcgvpfuT9ByH"
      },
      "source": [
        "# Visualizing the data\n",
        "\n",
        "plot_cols = ['CO2EQ', 'CO2', 'CH4', 'N2O','Temp_Anomaly']\n",
        "plot_features = df[plot_cols]\n",
        "plot_features.index = df['Years']\n",
        "_ = plot_features.plot(subplots=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Esh89owcMP4"
      },
      "source": [
        "# Splitting the dataframe for Training and Validation dataframes\n",
        "\n",
        "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "\n",
        "n = len(df)\n",
        "train_df = df[0:int(n*0.6)] \n",
        "val_df = df[int(n*0.6):int(n*0.68)] \n",
        "test_df = df[int(n*0.68):int(n)]\n",
        "\n",
        "num_features = df.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj67X30SPe0x"
      },
      "source": [
        "# check shape of training and validation dataframes\n",
        "\n",
        "print(train_df.shape)\n",
        "print(val_df.shape)\n",
        "print(test_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D01Zuif5PsU6"
      },
      "source": [
        "# Scaling the  data between 0 and 1 using minmaxscaler from keras\n",
        "# LSTM works better with scaled data\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "train = scaler.fit_transform(train_df) \n",
        "val = scaler.transform(val_df) \n",
        "test = scaler.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GM_4mXAQEnK"
      },
      "source": [
        "# split data into input features and targets\n",
        "\n",
        "train_x, train_y = train[:,:-1], train[:,-1] \n",
        "val_x, val_y = val[:,:-1], val[:,-1]\n",
        "test_x, test_y = test[:,:-1], test[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvXYbOJAQ6BA"
      },
      "source": [
        "# reshape train_x and test_x for the model\n",
        "\n",
        "train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\n",
        "\n",
        "val_x = val_x.reshape((val_x.shape[0], 1, val_x.shape[1]))\n",
        "\n",
        "test_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))\n",
        "\n",
        "print(train_x.shape, train_y.shape, val_x.shape, val_y.shape, test_x.shape, test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdzRHF9YUrmE"
      },
      "source": [
        "# building the LSTM model using keras\n",
        "\n",
        "def build_model(train_x):\n",
        "    inputs = keras.layers.Input(shape = (train_x.shape[1], train_x.shape[2]))\n",
        "    x = keras.layers.LSTM(50,return_sequences =  True)(inputs) \n",
        "    x = keras.layers.Dropout(0.3)(x) \n",
        "    x = keras.layers.LSTM(50, return_sequences = True)(x)\n",
        "    x = keras.layers.Dropout(0.3)(x)\n",
        "    x = keras.layers.LSTM(50)(x)\n",
        "    outputs = keras.layers.Dense(1, activation = 'linear')(x) \n",
        "\n",
        "    model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "    model.compile(optimizer = 'adam', loss = \"mse\") \n",
        "    return model\n",
        "\n",
        "model = build_model(train_x)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoV7il8ZVFYY"
      },
      "source": [
        "# fitting the model with model.fit without shuffling\n",
        "\n",
        "process = model.fit(train_x, train_y, epochs = 50, batch_size = 72, validation_data = (val_x, val_y), shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqXNf0foVOp9"
      },
      "source": [
        "# plotting the loss and validation loss against epochs\n",
        "\n",
        "def plot_process(process):\n",
        "    plt.plot(process.history['loss'], label='train')\n",
        "    plt.plot(process.history['val_loss'], label='validation')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_process(process)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fitPz_01Vj1K"
      },
      "source": [
        "def prediction(model,test_x,train_x, df):\n",
        "    # Predict using the model\n",
        "    predict =  model.predict(test_x)\n",
        "\n",
        "    # Reshape test_x and train_x for visualization and reshaping to original shape\n",
        "    test_x = test_x.reshape((test_x.shape[0], test_x.shape[2]))\n",
        "    train_x = train_x.reshape((train_x.shape[0], train_x.shape[2]))\n",
        "\n",
        "    # Concatenate test_x with predicted value\n",
        "    predict_ = np.concatenate((test_x, predict),axis = 1)\n",
        "\n",
        "    # Inverse-scaling to get the real values, removing min max scaler\n",
        "    predict_ = scaler.inverse_transform(predict_)\n",
        "    original_ = scaler.inverse_transform(test)\n",
        "\n",
        "    # Create dataframe to store the predicted and original values\n",
        "    pred = pd.DataFrame()\n",
        "    pred['Years'] = df['Years'][-test_x.shape[0]:]\n",
        "    pred['Original'] = original_[:,-1] \n",
        "    pred['Predicted'] = predict_[:,-1] \n",
        "\n",
        "    \n",
        "    # Create dataframe for visualization\n",
        "    df = df[['Years','Temp_Anomaly']][:-test_x.shape[0]]\n",
        "    df.columns = ['Years','Original']\n",
        "    original = df.append(pred[['Years','Original']])\n",
        "    df.columns = ['Years','Predicted']\n",
        "    predicted = df.append(pred[['Years','Predicted']])\n",
        "    original = original.merge(predicted, left_on = 'Years',right_on = 'Years')\n",
        "    return pred, original\n",
        "\n",
        "pred, original = prediction(model, test_x, train_x, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EoJkTwtaAia"
      },
      "source": [
        "\n",
        "\n",
        "# Plotting the predicted values\n",
        "\n",
        "def plot(df):\n",
        "    fig = px.line(title = 'Temperature Prediction')\n",
        "    fig.add_scatter(x = df['Years'], y = df['Original'], name = 'Recorded Temperature Anomaly', opacity = 0.7)\n",
        "    fig.add_scatter(x = df['Years'], y = df['Predicted'], name = 'Predicted Temperature Anomaly', opacity = 0.5)\n",
        "    fig.show()\n",
        "\n",
        "plot(original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g0Q9isKcvdd"
      },
      "source": [
        "# Model Validation using Radiative Forcing\n",
        "Here, we use the Radiative Forcing data set in the same model to validate how well the model is working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnP4AvVwdG3M"
      },
      "source": [
        "#Setting Up\n",
        "\n",
        "import os # import OS module to interact with Operating System\n",
        "import datetime # import module to manipulate date and time\n",
        "\n",
        "import IPython # import ipython command shell\n",
        "import IPython.display # import display tools\n",
        "\n",
        "import matplotlib as mpl # import matplot library for visualization\n",
        "import matplotlib.pyplot as plt # import library to plot timeseries data\n",
        "\n",
        "import numpy as np # import numpy library for mathematical operations\n",
        "import pandas as pd # import pandas library to process timeseries data\n",
        "import seaborn as sns # import seaborn library to process statistical graphics\n",
        "import tensorflow as tf # import tensorflow \n",
        "\n",
        "from tensorflow import keras # import Keras to build and summarize the model\n",
        "import plotly.express as px # import plotly.express to create graphs\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler # to normalize training data\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8,6) # setting the size of figures plotted using mpl\n",
        "mpl.rcParams['axes.grid'] = False # setting not to show gridlines on plots\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyTY8O0sdG3N"
      },
      "source": [
        "# Importing Data from computer \n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['Train.csv']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98PJW1mgdG3O"
      },
      "source": [
        "# Check Training Data\n",
        "\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6-EkTpLdG3P"
      },
      "source": [
        "# Visualizing the data\n",
        "\n",
        "plot_cols = ['CO2EQ', 'CO2', 'CH4', 'N2O','RF']\n",
        "plot_features = df[plot_cols]\n",
        "plot_features.index = df['Years']\n",
        "_ = plot_features.plot(subplots=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMvMWGSBdG3Q"
      },
      "source": [
        "# Splitting the dataframe into Training and Validation dataframes\n",
        "\n",
        "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "\n",
        "n = len(df)\n",
        "train_df = df[0:int(n*0.7)]\n",
        "val_df = df[int(n*0.7):int(n*0.9)]\n",
        "test_df = df[int(n*0.9):int(n)]\n",
        "num_features = df.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVy6IGgFdG3Q"
      },
      "source": [
        "# check shape of training and validation dataframes\n",
        "\n",
        "print(train_df.shape)\n",
        "print(val_df.shape)\n",
        "print(test_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvWo4_wUdG3Q"
      },
      "source": [
        "# Normalizing the training data between 0 and 1 using minmaxscaler from keras\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "train = scaler.fit_transform(train_df)\n",
        "val = scaler.transform(val_df)\n",
        "test = scaler.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ncab0SyIdG3R"
      },
      "source": [
        "# split data into input features and targets\n",
        "\n",
        "train_x, train_y = train[:,:-1], train[:,-1]\n",
        "val_x, val_y = val[:,:-1], val[:,-1]\n",
        "test_x, test_y = test[:,:-1], test[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mrztTx3dG3R"
      },
      "source": [
        "# reshape train_x and test_x for the model\n",
        "\n",
        "train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\n",
        "val_x = val_x.reshape((val_x.shape[0], 1, val_x.shape[1]))\n",
        "test_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))\n",
        "print(train_x.shape, train_y.shape, val_x.shape, val_y.shape, test_x.shape, test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmzZAAL9dG3R"
      },
      "source": [
        "# building the LSTM model using keras\n",
        "\n",
        "def build_model(train_x):\n",
        "    inputs = keras.layers.Input(shape = (train_x.shape[1], train_x.shape[2]))\n",
        "    x = keras.layers.LSTM(50,return_sequences =  True)(inputs)\n",
        "    x = keras.layers.Dropout(0.3)(x)\n",
        "    x = keras.layers.LSTM(50, return_sequences = True)(x)\n",
        "    x = keras.layers.Dropout(0.3)(x)\n",
        "    x = keras.layers.LSTM(50)(x)\n",
        "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "    model.compile(optimizer = 'adam', loss = \"mae\")\n",
        "    return model\n",
        "\n",
        "model = build_model(train_x)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fqS2T7OdG3S"
      },
      "source": [
        "# fitting the model with model.fit without shuffling\n",
        "\n",
        "process = model.fit(train_x, train_y, epochs = 50, batch_size = 72, validation_data = (val_x, val_y), shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k-x6xbxdG3S"
      },
      "source": [
        "# plotting the loss and validation loss against epochs\n",
        "\n",
        "def plot_process(process):\n",
        "    plt.plot(process.history['loss'], label='train')\n",
        "    plt.plot(process.history['val_loss'], label='validation')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_process(process)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9h3LbJhdG3T"
      },
      "source": [
        "def prediction(model,test_x,train_x, df):\n",
        "    # Predict using the model\n",
        "    predict =  model.predict(test_x)\n",
        "\n",
        "    # Reshape test_x and train_x for visualization  and inverse-scaling purpose\n",
        "    test_x = test_x.reshape((test_x.shape[0], test_x.shape[2]))\n",
        "    train_x = train_x.reshape((train_x.shape[0], train_x.shape[2]))\n",
        "\n",
        "    # Concatenate test_x with predicted value\n",
        "    predict_ = np.concatenate((test_x, predict),axis = 1)\n",
        "\n",
        "    # Inverse-scaling to get the real values\n",
        "    predict_ = scaler.inverse_transform(predict_)\n",
        "    original_ = scaler.inverse_transform(test)\n",
        "\n",
        "    # Create dataframe to store the predicted and original values\n",
        "    pred = pd.DataFrame()\n",
        "    pred['Years'] = df['Years'][-test_x.shape[0]:]\n",
        "    pred['Original'] = original_[:,-1]\n",
        "    pred['Predicted'] = predict_[:,-1]\n",
        "\n",
        "    # Calculate the error \n",
        "    pred['Error'] = pred['Original'] - pred['Predicted']\n",
        "    \n",
        "    # Create dataframe for visualization\n",
        "    df = df[['Years','RF']][:-test_x.shape[0]]\n",
        "    df.columns = ['Years','Original']\n",
        "    original = df.append(pred[['Years','Original']])\n",
        "    df.columns = ['Years','Predicted']\n",
        "    predicted = df.append(pred[['Years','Predicted']])\n",
        "    original = original.merge(predicted, left_on = 'Years',right_on = 'Years')\n",
        "    return pred, original\n",
        "\n",
        "pred, original = prediction(model, test_x, train_x, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkGV92MBdG3T"
      },
      "source": [
        "def plot(df):\n",
        "    # Plotting the Current and Predicted values\n",
        "    fig = px.line(title = 'Prediction vs. Actual')\n",
        "    fig.add_scatter(x = df['Years'], y = df['Original'], name = 'Original', opacity = 0.7)\n",
        "    fig.add_scatter(x = df['Years'], y = df['Predicted'], name = 'Predicted', opacity = 0.5)\n",
        "    fig.show()\n",
        "    \n",
        "plot(original)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}