{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Kopie von 3DCNN_HRN_jester_300_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SRa2FoFzJsd"
      },
      "source": [
        "**We mount our data from the google -Drive-repository.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw2RqksEgpfj",
        "outputId": "4dd610b2-9b66-42de-d43a-2765243dd51c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21wnvfHVziKJ"
      },
      "source": [
        "**Load the modules we need.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWIm_ZfzdSdf"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D,Conv2D,AveragePooling2D,AveragePooling3D\n",
        "from keras.layers import Dense, GlobalAveragePooling3D,GlobalAveragePooling2D\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler,ReduceLROnPlateau\n",
        "from keras.optimizers import SGD, RMSprop, Adadelta\n",
        "from keras.utils import np_utils, generic_utils\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
        "\n",
        "import theano\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# image specification\n",
        "img_rows,img_cols=125, 57 \n",
        "patch_size = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKLI3y1TzsEI"
      },
      "source": [
        "# Our gestures\n",
        "\n",
        "We only used 6 gestures, so the training wouldn't take forever.\n",
        "Notice, that the order is important, since we place the labels accordingly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR3xA0GqdSds"
      },
      "source": [
        "gestures = \"Rolling Hand Backward\", \"Rolling Hand Forward\", \"Stop\", \"Swiping Left\", \"Swiping Right\", \"No Gesture\"\n",
        "\n",
        "number_gestures = len(gestures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQa4uENvz7Yv"
      },
      "source": [
        "**The following cells will probably be of no use anymore since we load in the jester-set later and we now are able to load in the jester-set.**\n",
        "\n",
        "Here we just use the few gesture-videos we produced ourselves. Not of much use anymore since they're only 18ish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE3C0qEJdSdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a115bd1-91c7-4cd8-d89e-0794d21efcc7"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "gestures_count = np.zeros(number_gestures, dtype = int) # Count the number of each gesture\n",
        "\n",
        "X_tr=[]           # variable to store entire dataset\n",
        "# We now load in all picture-blocks at once.\n",
        "for gesture_index in range(number_gestures):\n",
        "  ls_path = os.path.join(\"/content/drive/MyDrive/generated_data/\", gestures[gesture_index])\n",
        "  listing = os.listdir(ls_path)\n",
        "\n",
        "  for ls in tqdm(listing):\n",
        "    listing_stop = sorted(os.listdir(os.path.join(ls_path,ls))) \n",
        "\n",
        "    frames = []\n",
        "    img_depth=0\n",
        "    for imgs in listing_stop:\n",
        "      if img_depth <16:\n",
        "        img = os.path.join(os.path.join(ls_path,ls),imgs)\n",
        "        frame = cv2.imread(img)\n",
        "        frame=cv2.resize(frame,(img_rows,img_cols),interpolation=cv2.INTER_AREA)\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(gray)\n",
        "        img_depth=img_depth+1\n",
        "      else:\n",
        "        break\n",
        "    input_img = np.array(frames)\n",
        "    ipt=np.rollaxis(np.rollaxis(input_img,2,0),2,0)\n",
        "    ipt=np.rollaxis(ipt,2,0)\n",
        "    X_tr.append(ipt)\n",
        "    gestures_count[gesture_index] += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:19<00:00,  6.45s/it]\n",
            "100%|██████████| 4/4 [00:25<00:00,  6.32s/it]\n",
            "100%|██████████| 2/2 [00:13<00:00,  6.55s/it]\n",
            "100%|██████████| 3/3 [00:19<00:00,  6.36s/it]\n",
            "100%|██████████| 4/4 [00:25<00:00,  6.26s/it]\n",
            "100%|██████████| 2/2 [00:12<00:00,  6.13s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70pGlelzdSeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d16acc0-d506-489c-8d0b-f7522c676702"
      },
      "source": [
        "X_tr_array = np.array(X_tr)   # convert the frames read into array\n",
        "print(X_tr_array.shape)\n",
        "#(num_samples, img_cols, img_rows ,num_pixels , num_colours)\n",
        "\n",
        "num_samples = len(X_tr_array) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18, 16, 57, 125, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ9jbTA2dSeJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162c2c0d-2117-4c7d-a0d1-3bd97b95397c"
      },
      "source": [
        "label=np.zeros(num_samples, dtype = int)\n",
        "\n",
        "\n",
        "## iterate through the gestures to create labels\n",
        "index = 0                             # run index\n",
        "for i in range(number_gestures):      # gestures\n",
        "  for j in range(gestures_count[i]):  # each video\n",
        "    label[index] = i                  # assign value\n",
        "    index += 1                        # adjust index\n",
        "print(label)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 1 1 1 1 2 2 3 3 3 4 4 4 4 5 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDXtDQ0-dSeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48db8e97-0075-4fa0-8bc6-fe84e0b67e88"
      },
      "source": [
        "img_depth = 16\n",
        "train_data = [X_tr_array,label]\n",
        "\n",
        "(X_train, y_train) = (train_data[0],train_data[1])\n",
        "print('X_Train shape:', X_train.shape)\n",
        "\n",
        "train_set = np.zeros((num_samples, img_depth, img_cols,img_rows,3))\n",
        "\n",
        "for h in range(num_samples):\n",
        "    train_set[h][:][:][:][:]=X_train[h,:,:,:]\n",
        "  \n",
        "\n",
        "patch_size = 16    # img_depth or number of frames used for each video\n",
        "\n",
        "print(train_set.shape, 'train samples')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_Train shape: (18, 16, 57, 125, 3)\n",
            "(18, 16, 57, 125, 3) train samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHbI5xNBdSeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35be1e86-4fa3-4a13-f22c-0dbe26930a0b"
      },
      "source": [
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, number_gestures)\n",
        "print(Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv2H_d6GdSeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a7d924-1a00-4980-df17-a3374a2546b6"
      },
      "source": [
        "# Pre-processing\n",
        "train_set = train_set.astype('float32')\n",
        "print(np.mean(train_set))             # locate around 0\n",
        "train_set -= np.mean(train_set)\n",
        "print(np.max(train_set))              # adjust to range\n",
        "train_set /=np.max(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "112.09022\n",
            "142.90979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I70yqba0jsn"
      },
      "source": [
        "# Our Model\n",
        "\n",
        "The model we used is already pretrained with 600 videos trained provided by the Jester-Dataset.\n",
        "We only have the Dense layers all followed by a dropout-layer. Everything before is just prepreprocessing of the images.\n",
        "\n",
        "The input for each prediction have to be 16 pictures concatenated by the way shown below.\n",
        "\n",
        "The code in the end allows to change the trainable layers by changing the attribute layer.trainable = True.\n",
        "\n",
        "Furthermore there is the possibility to change the Dropout-rate since we noticed the pretrained model to have a pretty huge dropout-rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji_JFBv6dSeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944ec880-6774-4817-eb61-4fabc94a96c7"
      },
      "source": [
        "import keras\n",
        "# Define model\n",
        "#high resolution network\n",
        "from keras import regularizers\n",
        "weight_decay = 0.005\n",
        "l2=keras.regularizers.l2\n",
        "patch_size = 16\n",
        "## We'll extract one layer to train it.\n",
        "name_layer_1 = \"dense_01\"\n",
        "name_layer_2 = \"dense_02\"\n",
        "name_layer_3 = \"dense_03\"\n",
        "\n",
        "# To tweek the dropout-rate\n",
        "name_dropout_1 = \"dropout_01\"\n",
        "name_dropout_2 = \"dropout_02\"\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv3D(4,(3,7,7),\n",
        "                 input_shape=(patch_size, img_cols, img_rows, 3),\n",
        "                 activation='relu',bias_initializer='ones'))\n",
        "model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
        "model.add(Conv3D(8,(3,5,5), activation='relu',bias_initializer='ones'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(Conv3D(32,(3,5,5), activation='relu',bias_initializer='ones'))\n",
        "model.add(MaxPooling3D(pool_size=(1, 1, 2)))\n",
        "model.add(Conv3D(64,(3,3,5), activation='relu',bias_initializer='ones'))\n",
        "model.add(MaxPooling3D(pool_size=(1,2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu',bias_initializer='ones', name=name_layer_1))\n",
        "model.add(Dropout(0.5,name = name_dropout_1))\n",
        "\n",
        "model.add(Dense(256, activation='relu',bias_initializer='ones', name=name_layer_2))\n",
        "model.add(Dropout(0.5,name = name_dropout_2))\n",
        "\n",
        "model.add(Dense(number_gestures,kernel_initializer='normal', name=name_layer_3))\n",
        "\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Added weights\n",
        "weight_path = r\"/content/drive/MyDrive/WS_Gesture-Recognition-with-3DCNN/save_model/3DCNN_HRN_300_6_jester\"\n",
        "model.load_weights(weight_path)\n",
        "\n",
        "# Only the first Dense layer is trainable\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "## CHOOSE THE LAYER YOU WANT TO TRAIN (name_layer_x)\n",
        "training_layer = model.get_layer(name_layer_1)\n",
        "training_layer.trainable = True\n",
        "\n",
        "\n",
        "# If you want to change the dropout rate, do this by\n",
        "dropout_layer = model.get_layer(name_dropout_1) # x being 1 or 2\n",
        "dropout_layer.rate = 0.5\n",
        "\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 14, 51, 119, 4)    1768      \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 14, 25, 59, 4)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 12, 21, 55, 8)     2408      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 6, 10, 27, 8)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 4, 6, 23, 32)      19232     \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 4, 6, 11, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 2, 4, 7, 64)       92224     \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 2, 2, 3, 64)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_01 (Dense)             (None, 512)               393728    \n",
            "_________________________________________________________________\n",
            "dropout_01 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_02 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_02 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_03 (Dense)             (None, 6)                 1542      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 642,230\n",
            "Trainable params: 393,728\n",
            "Non-trainable params: 248,502\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK46I-sG1u3N"
      },
      "source": [
        "I will remove this Cell since we don't use it. Probably is a stopping-device, if it doesn't train good enough"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7D7S2kTdSea"
      },
      "source": [
        "earlystop = EarlyStopping(monitor='val_loss', patience=50, verbose =1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgsJeUJM2ASJ"
      },
      "source": [
        "**Compile the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZnnn3FLdSeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed89bf3a-4121-4cdf-9c14-4de403d6b089"
      },
      "source": [
        "sgd = SGD(lr=0.001,  momentum=0.9, nesterov=False)\n",
        "rms = RMSprop(decay=1e-6)\n",
        "ada = Adadelta(lr=0.1,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer=sgd,\n",
        "              #optimizer=ada,\n",
        "              metrics=['acc'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W81DJdjt6k72"
      },
      "source": [
        "# Split the train_set into Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8UYiOyLdSec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1cd899-a708-47e0-9773-05b641261c83"
      },
      "source": [
        "# WE SPLIT INTO TRAIN- AND TESTDATA\n",
        "X_train_new, X_val_new, y_train_new,y_val_new = train_test_split(train_set, Y_train, test_size=0.2, random_state=20)\n",
        "print(X_train_new.shape)\n",
        "print(type(X_train_new))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14, 16, 57, 125, 3)\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBJV6S3Y7DAu"
      },
      "source": [
        "# Save the weights\n",
        "Here we can save and reload the weights produced by our training.\n",
        "First block for saving, second block for loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU6zKv0O0Ztd"
      },
      "source": [
        "# THIS BOX IS TO SAVE OUR WEIGHTS\n",
        "save_path = \"/content/drive/MyDrive/WS_Gesture-Recognition-with-3DCNN/save_model/trained\"\n",
        "for i in range(100):\n",
        "  save_path_new = os.path.join(save_path, str(i))       # We go through each possible weight-file-name\n",
        "  if not os.path.isfile(save_path_new):\n",
        "    model.save_weights(save_path_new)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASarq0Xg1Ebu"
      },
      "source": [
        "# RELOAD OUR LATEST WEIGHTS BY RUNNING THIS BOX:\n",
        "save_path = \"/content/drive/MyDrive/WS_Gesture-Recognition-with-3DCNN/save_model/trained\"\n",
        "for i in range(100):\n",
        "  save_path_new = os.path.join(save_path, str(100-i))   # We go through each possible weight-file-name BACKWARDS\n",
        "  if os.path.isfile(save_path_new):\n",
        "    model.load_weights(save_path_new)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZoAY1af7Uzm"
      },
      "source": [
        "# Will remove this as well, if everything works\n",
        "This is the little data-set we used in the beginning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "Nbmpabh63woN",
        "outputId": "12bd1024-6c1e-4ce6-a7ce-e0e2dcccf4dc"
      },
      "source": [
        "# CREATE THE TEST-SET X_val FROM JESTER\n",
        "\n",
        "X_val = []           # variable to store entire dataset\n",
        "# We now load in all picture-blocks at once.\n",
        "ls_path = os.path.join(\"/content/drive/MyDrive/\", \"training_samples7\")\n",
        "listing = os.listdir(ls_path)\n",
        "\n",
        "i = 1\n",
        "for ls in tqdm(listing):\n",
        "  if i < 10:        # How many images are loaded into X_val\n",
        "    listing_stop = sorted(os.listdir(os.path.join(ls_path,ls))) \n",
        "\n",
        "    frames = []\n",
        "    img_depth=0\n",
        "    for imgs in listing_stop:\n",
        "      if img_depth <16:\n",
        "        img = os.path.join(os.path.join(ls_path,ls),imgs)\n",
        "        frame = cv2.imread(img)\n",
        "        frame=cv2.resize(frame,(img_rows,img_cols),interpolation=cv2.INTER_AREA)\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(gray)\n",
        "        img_depth=img_depth+1\n",
        "      else:\n",
        "        break\n",
        "    input_img = np.array(frames)\n",
        "    ipt=np.rollaxis(np.rollaxis(input_img,2,0),2,0)\n",
        "    ipt=np.rollaxis(ipt,2,0)\n",
        "    X_val.append(ipt)\n",
        "    i += 1\n",
        "\n",
        "\n",
        "X_val_array = np.array(X_val)   # convert the frames read into array\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a73f7eee3d65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlisting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m        \u001b[0;31m# How many images are loaded into X_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlisting_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKWQp69s7eBs"
      },
      "source": [
        "Here you can predict some gestures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLejM79gP-da",
        "outputId": "4f7194ea-f4ce-4ca4-b6d0-ecbc74f551ae"
      },
      "source": [
        "test_pred = model.predict(X_val_array[0:9])\n",
        "result = np.argmax(test_pred, axis =1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 1 4 2 2 1 3 4 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z8porcf7mXi"
      },
      "source": [
        "# Validation\n",
        "\n",
        "Here we load the videos we produced ourselves. These are about 10*6 Videos. We wanted to produce videos for each gesture in the same scenario, so we could see the problems of our model better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7b_seB0yvX9"
      },
      "source": [
        "# CREATE THE VALIDATION SET\n",
        "# Val will be in the form:\n",
        "# 1_geste_0, 2_geste_0,..., 1_geste_1, 2_geste_1,...,1_geste_2,2_geste2,... \n",
        "Val = []\n",
        "gesture_begin = 1 # We take 16 pictures from gesture_begin onwards\n",
        "\n",
        "ls_path = os.path.join(\"/content/drive/MyDrive\", \"validation\")\n",
        "listing = os.listdir(ls_path)\n",
        "number_scenes = len(listing)            # Count, how many video-scenarios there are\n",
        "Val_y = np.zeros(6*number_scenes)       # Hopefully, we have 6 videos in each video-scenario # Labels\n",
        "for i in range(6):                      # Go through each gesture\n",
        "    for j in range(number_scenes):        # Go through each scene\n",
        "      Val_y[i*number_scenes + j] = i      # We go through gesture after gesture and create the labels\n",
        "      video_path = os.path.join(ls_path, str(j+1), str(j+1) + \"_\" + \"geste\" + \"_\" + str(i)) # These are the videos\n",
        "      frames = []\n",
        "      img_depth = 0\n",
        "      for imgs in sorted(os.listdir(video_path)):\n",
        "        if gesture_begin <= img_depth and img_depth < gesture_begin + 16:\n",
        "          img = os.path.join(video_path, imgs)\n",
        "          frame = cv2.imread(img)\n",
        "          frame = cv2.resize(frame, (img_rows, img_cols), interpolation=cv2.INTER_AREA)\n",
        "          gray = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "          frames.append(gray)\n",
        "          img_depth += 1\n",
        "        elif img_depth < gesture_begin:\n",
        "          img_depth += 1\n",
        "        else:\n",
        "          break\n",
        "      input_img = np.array(frames)\n",
        "      ipt = np.rollaxis(np.rollaxis(input_img,2,0),2,0)\n",
        "      ipt = np.rollaxis(ipt,2,0)\n",
        "      Val.append(ipt)\n",
        "\n",
        "\n",
        "\n",
        "Val_array = np.array(Val)   # convert the frames read into array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8B2GmV88HEd"
      },
      "source": [
        "# Produce the prediction-array\n",
        "\n",
        "Every row stands for one gesture, every column for one scenario (same person, same background, same shirt).\n",
        "\n",
        "So the perfect prediction would be:\n",
        "\n",
        "[[ 0 0 0 0 0 0...]\n",
        "\n",
        "[1 1 1 1 1 1 ...]\n",
        "\n",
        "[2 2 2 2 2 2 ...]\n",
        "\n",
        "...\n",
        "\n",
        "[5 5 5 5 5 5 ...] ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE_-t0r5WYXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daed829d-178f-4020-a984-fdc08bc52d13"
      },
      "source": [
        "val_pred = model.predict(Val_array[:])\n",
        "result_val = np.argmax(val_pred, axis=1)\n",
        "result_val = result_val.reshape(6, number_scenes)\n",
        "print(result_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 4 0 3 0 1 1]\n",
            " [0 1 4 0 1 4 1 1]\n",
            " [2 2 4 2 0 3 3 1]\n",
            " [0 0 4 3 3 3 2 3]\n",
            " [4 4 4 3 4 4 2 1]\n",
            " [2 4 2 4 5 4 1 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN0d9TAw84xd"
      },
      "source": [
        "# Check, which ones are correct\n",
        "In this block, we create the correct prediction matrix and compare with the prediction made by our model.\n",
        "\n",
        "A 1 stands for a correct prediction, a 0 for a false prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdU2WxyOqrY-",
        "outputId": "db78a8ba-01dd-46e7-dab6-46e98308c55c"
      },
      "source": [
        "# Check, which is correct\n",
        "# AUFHÜBSCHEN\n",
        "A = np.zeros([6, number_scenes])\n",
        "a = np.ones(number_scenes)\n",
        "\n",
        "for i in range(6):\n",
        "  A[i,:] += i * a\n",
        "\n",
        "check_matrix = (A == result_val)\n",
        "check_matrix = check_matrix * 1\n",
        "print(check_matrix)\n",
        "print(\"Sum of all correct predictions is\", np.sum(check_matrix), \"of\", number_scenes*number_gestures)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 1 0 1 0 0]\n",
            " [0 1 0 0 1 0 1 1]\n",
            " [1 1 0 1 0 0 0 0]\n",
            " [0 0 0 1 1 1 0 1]\n",
            " [1 1 1 0 1 1 0 0]\n",
            " [0 0 0 0 1 0 0 1]]\n",
            "Sum of all correct predictions is 21 of 48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSmW6TQQ9wIu"
      },
      "source": [
        "# Create the best beginning\n",
        "\n",
        "Since we only load 16 images per video, there will be a part of the video cut off. In this cell we experimentally figured out, how many images in the beginning to crop from the beginning of the videos created by ourselves so it would make the best predictions for this data-set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfil04e4z1Pn",
        "outputId": "0f793fb6-f8c5-400f-f2d2-ecfa02543940"
      },
      "source": [
        "# THIS CODE-BLOCK IS TO FIND OUT THE MOST BEST GESTURE_BEGIN_RATE\n",
        "# CREATE THE VALIDATION SET\n",
        "# Val will be in the form:\n",
        "# 1_geste_0, 2_geste_0,..., 1_geste_1, 2_geste_1,...,1_geste_2,2_geste2,... \n",
        "\n",
        "first_first_frame = 0\n",
        "last_first_frame = 12\n",
        "count = np.zeros(last_first_frame-first_first_frame)\n",
        "\n",
        "ls_path = os.path.join(\"/content/drive/MyDrive\", \"validation\")\n",
        "listing = os.listdir(ls_path)\n",
        "number_scenes = len(listing)            # Count, how many video-scenarios there are\n",
        "\n",
        "\n",
        "for gesture_begin in range(first_first_frame, last_first_frame):\n",
        "  Val = []\n",
        "  Val_y = np.zeros(6*number_scenes)       # Hopefully, we have 6 videos in each video-scenario # Labels\n",
        "  for i in range(6):                      # Go through each gesture\n",
        "    for j in range(number_scenes):        # Go through each scene\n",
        "      Val_y[i*number_scenes + j] = i      # We go through gesture after gesture and create the labels\n",
        "      video_path = os.path.join(ls_path, str(j+1), str(j+1) + \"_\" + \"geste\" + \"_\" + str(i)) # These are the videos\n",
        "      frames = []\n",
        "      img_depth = 0\n",
        "      for imgs in sorted(os.listdir(video_path)):\n",
        "        if gesture_begin <= img_depth and img_depth < gesture_begin + 16:\n",
        "          img = os.path.join(video_path, imgs)\n",
        "          frame = cv2.imread(img)\n",
        "          frame = cv2.resize(frame, (img_rows, img_cols), interpolation=cv2.INTER_AREA)\n",
        "          gray = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "          frames.append(gray)\n",
        "          img_depth += 1\n",
        "        elif img_depth < gesture_begin:\n",
        "          img_depth += 1\n",
        "        else:\n",
        "          break\n",
        "      input_img = np.array(frames)\n",
        "      ipt = np.rollaxis(np.rollaxis(input_img,2,0),2,0)\n",
        "      ipt = np.rollaxis(ipt,2,0)\n",
        "      Val.append(ipt)\n",
        "\n",
        "\n",
        "  Val_array = np.array(Val)   # convert the frames read into array\n",
        "  val_pred = model.predict(Val_array[:])\n",
        "  result_val = np.argmax(val_pred, axis=1)\n",
        "  result_val = result_val.reshape(6, number_scenes)\n",
        "\n",
        "  A = np.zeros([6, number_scenes])\n",
        "  a = np.ones(number_scenes)\n",
        "\n",
        "  for i in range(6):\n",
        "    A[i,:] = i * a\n",
        "\n",
        "  check_matrix = (A == result_val)\n",
        "  check_matrix = check_matrix * 1\n",
        "  count[gesture_begin-first_first_frame] = np.sum(check_matrix)\n",
        "\n",
        "print(count)\n",
        "print(\"Best begin seems to be\", np.argmax(count)+first_first_frame)\n",
        "print(\"So we have\", np.max(count), \"correct predictions\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19. 21. 20. 19. 19. 19. 18. 17. 14. 16. 14. 10.]\n",
            "Best begin seems to be 1\n",
            "So we have 21.0 correct predictions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsiTYKZ3-pfY"
      },
      "source": [
        "# Create our training set\n",
        "\n",
        "Here we loaded some "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJGXyvEkB9Q-"
      },
      "source": [
        "\n",
        "def get_labels_from_csv(csv_path):\n",
        "  gestures = [\"Rolling Hand Backward\", \"Rolling Hand Forward\", \"Stop Sign\", \"Swiping Left\", \"Swiping Right\", \"No gesture\"]\n",
        "\n",
        "  with open(csv_path) as csv_file:\n",
        "    labels = []\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    i = 0\n",
        "    for line in csv_reader:\n",
        "      if i:\n",
        "        if line:\n",
        "          # print(line)\n",
        "          if line[1] in gestures:\n",
        "            labels.append([line[0], gestures.index(line[1])])  \n",
        "      i+=1\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSX9-xNGybi3"
      },
      "source": [
        "\n",
        "In the following Code-Block, we use the extracted datafiles from Jester and load them into the variable X_val. \n",
        "\n",
        "The Labels to our Data, we get by the function get_labels_from_cv províded in the cell above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On864iwHO6pr"
      },
      "source": [
        "# CREATE THE Training-SET X_val FROM JESTER\n",
        "\n",
        "import csv\n",
        "X_val = []           # variable to store entire dataset\n",
        "Y_val = []           # Create the labels\n",
        "ls_path = \"/content/drive/MyDrive/training_samples7\"\n",
        "ls = get_labels_from_csv(\"/content/drive/MyDrive/training_samples8train.csv\")\n",
        "\n",
        "num_samples = len([row[0] for row in ls])\n",
        "for i in range(num_samples):\n",
        "  listing_stop = sorted(os.listdir(os.path.join(ls_path,ls[i][0]))) # Die einzelnen Bilder\n",
        "  # print(\"Listing_stop sieht so aus: \", listing_stop)\n",
        "  # print(\"Listeneintrag\", ls[i][0])\n",
        "  # print(listing_stop) \n",
        "  frames = []\n",
        "  img_depth=0\n",
        "  for imgs in listing_stop:\n",
        "    # print(listing_stop)\n",
        "    # print(\"imgs ist \",  imgs)\n",
        "    # print(\"die datei, die man einladen will heißt \", os.path.join(os.path.join(ls_path,ls[i][0]), imgs))\n",
        "    if img_depth <16:\n",
        "      img = os.path.join(os.path.join(ls_path,ls[i][0]), imgs)\n",
        "      # print(img)\n",
        "      frame = cv2.imread(img)\n",
        "      # print(frame)\n",
        "      frame = cv2.resize(frame,(img_rows,img_cols),interpolation=cv2.INTER_AREA)\n",
        "      gray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "      frames.append(gray)\n",
        "      img_depth=img_depth+1\n",
        "    else:\n",
        "      break\n",
        "  input_img = np.array(frames)\n",
        "  ipt=np.rollaxis(np.rollaxis(input_img,2,0),2,0)\n",
        "  ipt=np.rollaxis(ipt,2,0)\n",
        "  X_val.append(ipt)\n",
        "  Y_val.append(ls[i][1])\n",
        "\n",
        "\n",
        "X_val_array = np.array(X_val)   # convert the frames read into array\n",
        "Y_val_array = np.array(Y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjfMGFsVudXg",
        "outputId": "c43508c5-6e23-492d-dace-253fc1064a07"
      },
      "source": [
        "\n",
        "# NEXT CELL\n",
        "img_depth = 16\n",
        "train_data = [X_val_array,Y_val_array]\n",
        "\n",
        "# print(X_val_array.shape, Y_val_array.shape)\n",
        "(X_train, y_train) = (train_data[0],train_data[1])\n",
        "# print(X_train.shape, y_train)\n",
        "\n",
        "train_set = np.zeros((num_samples, img_depth, img_cols,img_rows,3))\n",
        "\n",
        "for h in range(num_samples):\n",
        "    train_set[h][:][:][:][:]=X_train[h,:,:,:]\n",
        "  \n",
        "patch_size = 16    # img_depth or number of frames used for each video\n",
        "\n",
        "\n",
        "# NEXT CELL\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, number_gestures)\n",
        "\n",
        "\n",
        "# NEXT CELL\n",
        "# Pre-processing\n",
        "train_set = train_set.astype('float32')\n",
        "print(np.mean(train_set))             # locate around 0\n",
        "train_set -= np.mean(train_set)\n",
        "print(np.max(train_set))              # adjust to range\n",
        "train_set /=np.max(train_set)\n",
        "\n",
        "\n",
        "# NEXT CELL\n",
        "# WE SPLIT INTO TRAIN- AND TESTDATA\n",
        "X_train_new, X_val_new, y_train_new,y_val_new = train_test_split(train_set, Y_train, test_size=0.1, random_state=20)\n",
        "print(X_train_new.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "108.3591\n",
            "146.6409\n",
            "(1019, 16, 57, 125, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t2aNzcG65A1"
      },
      "source": [
        "#Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMFX-6AudSee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdabb102-4a4f-436d-dece-b65437c63493"
      },
      "source": [
        "batch_size = 200\n",
        "nb_epoch = 100\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.00001, \n",
        "                               cooldown=0, patience=2, min_lr=0.005/(2^4))\n",
        "\n",
        "hist = model.fit(\n",
        "    X_train_new,\n",
        "    y_train_new,\n",
        "    validation_data=(X_val_new,y_val_new),\n",
        "    batch_size=batch_size,\n",
        "    epochs = nb_epoch,\n",
        "    shuffle=True,\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r1/6 [====>.........................] - ETA: 2:48 - loss: 0.7480 - acc: 0.6950"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuwt40zZdSef"
      },
      "source": [
        "training_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "plt.plot(training_loss, label=\"training_loss\")\n",
        "plt.plot(val_loss, label=\"validation_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdOotijgdSeh"
      },
      "source": [
        "training_acc = hist.history['acc']\n",
        "val_acc = hist.history['val_acc']\n",
        "\n",
        "plt.plot(training_acc, label=\"training_accuracy\")\n",
        "plt.plot(val_acc, label=\"validation_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}