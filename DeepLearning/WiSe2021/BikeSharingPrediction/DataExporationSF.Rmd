---
title: "R Notebook"
output: html_notebook
---

### Project bike sharing Sprottenflotte ###
### WS 2020/2021 Course Deep Learning
### Project of Andrej Ponomarenko and Daniel Michels
### We used RStudio (R and Python) and Python in Colab for the project.

### RStudio project structure:
### DataExporationSF.Rmd: Exploring and transforming the dataset and base-line models naive prediction and linear model. 
### NeuralNetDataPreparation.R: Importing prepared dataset from above and defining dummy variables, normalization of variables, defining features and labels, splitting in training data and testing data
### NeuralNetEstimation_Model.Rmd: Neural Net Model for all stations dataset.
### NeuralNetEstimation_ModelHBFUmsteiger.Rmd: Neural Net Model for the stations Main Station and Umsteiger (HBF and Umsteiger) dataset.

### Sprottenflotte has given us a dataset of nearly 100.000 discrete bike renting and 
### returning events in a time frame ranging from 2019-07-03 to 2020-03-31. 

### Results of Project for bikes rented
|Dataset      |Naive Prediction MSE|Linear Model MSE| Neural Net MSE|
|-------------|--------------------|----------------|---------------|
|All stations |                1407|             967|            654|
|HBF Umsteiger|                  62|               -|             31| 

### Importing of relevant packages

```{r}
library(readr)
library(tidyverse)
library(readxl)
library(lubridate)
library(dplyr)
library(smooth)
library(ggplot2) 
library(dygraphs) #interactive timeseries graphs
library(xts)
library(Metrics)
library(zoo)
library(igraph) #network graphics / diagrams
```

### Import of the original dataset and the GPS-data of the stations


```{r}
SprottenFlotte_Dataset <- read_excel("E:/ManuIhmchen/OpenCampus/DeepLearningOC/SprottenFlotte/D04/Ausleihen_R++ckgaben_SprottenFlotte.xlsx",
    sheet = "Tabelle1")
SF_GPS_data<-read_excel("E:/ManuIhmchen/OpenCampus/DeepLearningOC/SprottenFlotte/D04/Ausleihen_R++ckgaben_SprottenFlotte.xlsx", 
    sheet = "Koordinaten")
View(SF_GPS_data)
View(SprottenFlotte_Dataset)

# Filter out all incomplete datasets with one or more columns containing NA
SF_clean <- SprottenFlotte_Dataset[complete.cases(SprottenFlotte_Dataset), ]
#SF_clean$time_rental<-as.POSIXlt(SF_clean$time_rental)
#SF_clean$time_return<-as.POSIXlt(SF_clean$time_return)
```

### Helper function to calculate the haversine distance between two stations given the GPS coordinates of the stations.

```{r}
haversine_distance <- function(lat1=0.0,lon1=0.0,lat2=0.0,lon2=0.0) {
    radius <- 6371 # in km
    x1 <- lat2 - lat1
    dLat <- x1 *(pi/180)
    x2 <- lon2 - lon1
    dLon <- x2*(pi/180)
    a <- sin(dLat / 2) * sin(dLat / 2) +
        cos(lat1 *(pi/180)) * cos(lat2 *(pi/180)) *
        sin(dLon / 2) * sin(dLon / 2);
    c <- 2 * atan2(sqrt(a), sqrt(1 - a))
    d <- radius * c
    return(d*1.0)
}
```

### Removing of data from environment if necessary

```{r}
rm(SF_clean, SF_clean_test, SF_GPS_data)
```

### Calculation the distance between stations using the delivered GPS-data of stations

```{r}
# Splitting up GPS information per station into latitude and longitude 
my_koordinates<-as_tibble(str_split(SF_GPS_data$`GPS Koordinaten`,",", simplify = TRUE))
my_koordinates<-my_koordinates %>%
  mutate(latitude=as.numeric(V1)) %>%
  mutate(longitude=as.numeric(V2))

# GPS data for station, where the bike is rented
SF_GPS_data_rent <- SF_GPS_data %>%
  separate(`GPS Koordinaten`,into = c("latitude1","longitude1"), sep=",", convert=TRUE) %>%
  mutate(station_nr_rental=Stationsummer) %>%
  select(-c(Stationsname,Stationsummer))

# GPS data of station where the bike is returned
SF_GPS_data_return <- SF_GPS_data %>%
  separate(`GPS Koordinaten`,into = c("latitude2","longitude2"), sep=",", convert=TRUE) %>%
  mutate(station_nr_return=Stationsummer) %>%
  select(-c(Stationsname,Stationsummer))
  
```

### Design of the data-frame used for prediction

```{r}
#SF_clean$time_rental<- as.Date(SF_clean$time_rental, format="%Y-%m-%s %h:%m:%s")
#SF_clean$time_return<- as.Date(SF_clean$time_return, format="%Y-%m-%s %h:%m:%s")
#SF_clean$time_rental<- as_datetime(ymd_hms(SF_clean$time_rental))
#SF_clean$time_return<- as_datetime(ymd_hms(SF_clean$time_return,tz = "UTC"))

# bus driver strikes are a very important issue with a high impact on demand for bike renting
StreikTage<-c(as_date("2020-01-14"),as_date("2020-01-31"),as_date("2020-02-01"),as_date("2020-02-11"),as_date("2020-02-12"),as_date("2020-02-13"))

# christmas seems to be relevant resulting in a much lower demand for bike sharing
Weihnachten<-c(as_date("2019-12-24"),as_date("2019-12-25"),as_date("2019-12-26"))

# Setting up the time interval for the data-frame (here 6 hours)
DateTimeSeq<-seq(ISOdate(2019,7,2,tz="UTC"), ISOdate(2020,4,1), by = "6 hour") # or "6 hours"

# Building the initial data-frame SF_clean calculating the date of rent and return and the rent_time, including the information of bus driver strike and christmas
SF_clean <- SF_clean %>%
  mutate(day_rental=as_date(time_rental)) %>%
  mutate(day_return=as_date(time_return)) %>%
  mutate(rent_time=time_return-time_rental) %>%
  mutate(is_streiktag=if_else(day_rental %in% StreikTage,1,0)) %>%
  mutate(is_weihnachten=if_else(day_rental %in% Weihnachten,1,0))

# Including weekdays
SF_clean$Wochentag<-weekdays(SF_clean$day_rental)
SF_clean <- SF_clean %>%
  mutate(is_wochenende=if_else(Wochentag %in% c("Samstag","Sonntag"),1,0))

# Including the GPS data
SF_clean <- SF_clean %>%
  left_join(SF_GPS_data_rent,by="station_nr_rental") %>%
  left_join(SF_GPS_data_return, by="station_nr_return")

# Calculation of the haversine distance unsing the GPS-data
SF_clean <- SF_clean %>%
  mutate(distance=haversine_distance(lat1=latitude1,lon1=longitude1,lat2=latitude2,lon2=longitude2))

# Calculating the time-interval for renting and returning the bike
SF_clean_test <- SF_clean %>%
  mutate(rentinterval=lubridate::interval(start=day_rental+hours(6*(hour(time_rental)%/%6)),end=day_rental+hours(6*(1+hour(time_rental)%/%6)))) %>%
  mutate(returninterval=lubridate::interval(start=day_return+hours(6*(hour(time_return)%/%6)),end=day_return+hours(6*(1+hour(time_return)%/%6))))


```

### Analysis of SF_clean (time scale of 1 day)

```{r}
SF_group_rentdate<- SF_clean %>%
  group_by(day_rental)
SF_rents_per_day<-SF_group_rentdate %>%
  summarise(
    rent_time=mean(rent_time),
    meandist=mean(distance),
    maxdist=max(distance),
    Wochentag=max(Wochentag),
    is_streiktag=max(is_streiktag),
    is_weihnachten=max(is_weihnachten),
    n=n()
  )
ggplot(SF_rents_per_day, aes(day_rental,n)) +
  geom_point()
```

### Exploring the pivot of station connections used for renting and returning

```{r}
SF_group_station <- SF_clean %>%
  group_by(Station_rental, station_return) %>%
  summarise(
    rent_time=mean(rent_time),
    n=n()
  ) %>%
  pivot_wider(Station_rental,names_from=station_return, values_from=n, values_fill=0)
#write.csv(SF_group_station,"Daten/StationStationPivot.csv")
#my_connect_mat<-data.matrix(SF_group_station)
#my_connect_mat<-my_connect_mat[,-c(1,43)]
# build the graph object
#network <- graph_from_adjacency_matrix(my_connect_mat)
 
# plot it
#plot(network)
```
### Overview of all stations and bikes rented per day 

```{r}
SF_group_rentdatestation<- SF_clean %>%
  group_by(day_rental,Station_rental)
SF_rents_per_stationperday<-SF_group_rentdatestation %>%
  summarise(
    n=n()
  )
SF_pivot<-SF_rents_per_stationperday %>%
  pivot_wider(names_from = Station_rental, values_from = n)
ggplot(SF_rents_per_stationperday, aes(day_rental,Station_rental, fill= n)) + 
  geom_tile()
#plot(SF_rents_per_stationperday$day_rental,SF_rents_per_day$n)
```
### Special analysis for station "Umsteiger" (a very frequently used station)

```{r}
SF_Umsteiger<-SF_clean %>% 
  filter(station_nr_rental==20029 | station_nr_return==20029)
SF_Umsteiger<-SF_Umsteiger %>%
  mutate(g = case_when((station_nr_rental==20029 & station_nr_return==20029) ~ 0,
         station_nr_rental==20029 ~ -1, station_nr_return == 20029 ~ 1))
SF_Umsteiger_Summary<-SF_Umsteiger %>%
  group_by(day_rental) %>%
  summarize(
    rent_time=mean(rent_time),
    bikes_moved=sum(g),
    n=n()
  )
#plot(SF_Umsteiger_Summary$day_rental,SF_Umsteiger_Summary$n)
#plot(SF_Umsteiger_Summary$day_rental,SF_Umsteiger_Summary$bikes_moved)
ggplot(SF_Umsteiger_Summary, aes(day_rental, n)) +
  geom_point()
ggplot(SF_Umsteiger_Summary, aes(day_rental, bikes_moved))+
  geom_point()

```


### Analysis of SF_clean_test (Intraday timeinterval of x hours)

```{r}
SF_clean_test <- SF_clean_test %>%
  mutate(start_rentinterval=rentinterval$start) %>%
  mutate(end_rentinterval=returninterval$start)
SF_group_rentinterval<- SF_clean_test %>%
  group_by(start_rentinterval)
SF_rents_per_interval<-SF_group_rentinterval %>%
  summarise(
    rent_time=mean(rent_time),
    Wochentag=max(Wochentag),
    n=n()
  )
ggplot(SF_rents_per_interval, aes(start_rentinterval,n)) +
  geom_point()
```

### Present the above data as interactive time-series graph
### for watching the graph the graph has to be saved to the file-system (see lines after save the widget)

```{r}
don <- xts(x = SF_rents_per_interval$n, order.by = SF_rents_per_interval$start_rentinterval)
p <- dygraph(don) %>%
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.1, drawGrid = FALSE, colors="#D8AE5A") %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 5, highlightSeriesBackgroundAlpha = 0.2, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1)

# save the widget
#library(htmlwidgets)
#saveWidget(p, file=paste0( getwd(), "/HtmlWidget/rentsperinterval.html"))

```


### Building a dataset for the "Umsteiger" station (example)

```{r}
# Build the data-frame based on the chosen time interval
SF_Umsteiger_interval<-as_tibble(DateTimeSeq) %>%
  rename(interval=value) %>% 
  mutate(Wochentag=weekdays(interval))

# Data-frame with bikes-rented
SF_Umsteiger_rentals<-SF_clean_test %>% 
  filter(station_nr_rental==20029) %>%
  mutate(category="Rent") %>%
  mutate(interval=start_rentinterval) %>%
  mutate(effect=-1)

# Data-frame with bikes-returned
SF_Umsteiger_returns<-SF_clean_test %>%
  filter(station_nr_return==20029) %>%
  mutate(interval=end_rentinterval) %>%
  mutate(category="Return") %>%
  mutate(effect=1)

# Combine the bikes_rented and bikes renturned
SF_Umsteiger_complete<- bind_rows(SF_Umsteiger_rentals,SF_Umsteiger_returns)

# Summarise the dataset by time-interval
SF_Umsteiger_Grouped<-SF_Umsteiger_complete %>%
  group_by(interval) %>%
  dplyr::summarise(
    rent_time=mean(rent_time),
    dist=mean(distance),
    maxdist=max(distance),
    bikes_diff=sum(effect),
    bikes_rented=sum(effect[category=="Rent"]),
    bikes_returned=sum(effect[category=="Return"]),
    n=n()
  )
#Fill in missing intervals
SF_Umsteiger_Grouped_AllIntervals<-left_join(SF_Umsteiger_interval,SF_Umsteiger_Grouped,by="interval")

#Fill missing data with 0
SF_Umsteiger_Grouped_AllIntervals<-SF_Umsteiger_Grouped_AllIntervals %>%
  replace_na(list(rent_time=0, bikes_diff=0,bikes_rented=0,bikes_returned=0, n=0))

#SF_Umsteiger_complete<- full_join(SF_Umsteiger_complete,SF_Umsteiger_returns,by="interval")

#plot(SF_Umsteiger_Summary$day_rental,SF_Umsteiger_Summary$n)
#plot(SF_Umsteiger_Summary$day_rental,SF_Umsteiger_Summary$bikes_moved)
# ggplot(SF_UmsteigerIV_Summary, aes(start_rentinterval, n)) + 
#   geom_point()
# ggplot(SF_UmsteigerIV_Summary, aes(start_rentinterval, bikes_moved))+
#   geom_point()
  
```
### Analyse which stations are part of the sharing net and when did the station join the net

```{r}
SF_group_station <- SF_clean_test %>%
  group_by(station_nr_rental, Station_rental) %>%
  dplyr::summarise(
    first_rent_date=min(day_rental),
    n=n()
  )
```
### This function helps for building the desired data-frame for a single station or a set of stations

```{r}
myStationAnalyzer <- function(station_nbr=c(20029)){
  # Build the data-frame based on the chosen time interval
  my_interval<-as_tibble(DateTimeSeq) %>%
    rename(interval=value) %>% 
    mutate(Wochentag=weekdays(interval))
  
  # Build a data-frame for the rented bikes
  my_rentals<-SF_clean_test %>% 
    filter(station_nr_rental %in% station_nbr) %>%
    mutate(category="Rent") %>%
    mutate(interval=start_rentinterval) %>%
    mutate(effect=-1)
  
  # Build a data-frame for the returned bikes
  my_returns<-SF_clean_test %>%
    filter(station_nr_return %in% station_nbr) %>%
    mutate(interval=end_rentinterval) %>%
    mutate(category="Return") %>%
    mutate(effect=1)
  
  # Put rented and returned data-frames together
  my_complete<- bind_rows(my_rentals,my_returns)
  
  # Group the data-frame per time-interval and summarise it
  my_Grouped<-my_complete %>%
    group_by(interval) %>%
    dplyr::summarise(
      rent_time=mean(rent_time),
      dist=mean(distance),
      maxdist=max(distance),
      bikes_diff=sum(effect),
      bikes_rented=sum(effect[category=="Rent"]),
      bikes_returned=sum(effect[category=="Return"]),
      n=n()
    )
  
  #Fill in missing intervals
  my_Grouped_AllIntervals<-left_join(my_interval,my_Grouped,by="interval")
  
  #Fill missing data with 0 and enrich by categorical data (christmas, weekday, bus driver strike day and weekend)
  my_Grouped_AllIntervals<-my_Grouped_AllIntervals %>%
    replace_na(list(rent_time=0, bikes_diff=0,bikes_rented=0,bikes_returned=0, dist=0, maxdist=0, n=0)) %>%
    mutate(station=paste( unlist(station_nbr), collapse=',')) %>%
    mutate(is_streiktag=if_else(as_date(interval) %in% StreikTage, 1, 0)) %>%
    mutate(is_weihnachten=if_else(as_date(interval) %in% Weihnachten, 1, 0)) %>%
    mutate(is_wochenende=if_else(Wochentag %in% c("Samstag","Sonntag"),1,0))
  return(my_Grouped_AllIntervals)
}
```

### Build the desired data-frames for several set of stations

```{r}
hbf_umsteiger<-myStationAnalyzer(c(20016,20029))
mygrp_tibble<-hbf_umsteiger %>%
  group_by(station) %>%
  dplyr::summarise(
    bikes_rented=sum(bikes_rented),
    bikes_returned=sum(bikes_returned)
  )

kirchhofallee<-myStationAnalyzer(c(20020))
Westring<-myStationAnalyzer(c(20032))
Wilhelmsplatz<-myStationAnalyzer(c(20033))
AlterMarkt<-myStationAnalyzer(c(20033))
AlleStationen<-myStationAnalyzer(SF_group_station$station_nr_rental)
```

### Time Series analysis of some station or station sets using partial auto correlation function.

```{r}
pacf(kirchhofallee$bikes_rented)
pacf(kirchhofallee$bikes_returned)
pacf(hbf_umsteiger$bikes_rented)
pacf(hbf_umsteiger$bikes_returned)
pacf(AlleStationen$bikes_rented)
pacf(AlleStationen$bikes_returned)
```
### Build the input data set for linear modelling and neural nets for "AllStations" based on exploration of data and time-series analysis (lagged variables for 1 week: 4 time steps each day * 7 days = 28 )
### Naive prediction as one base-line model prediction for time_interval t = value of 1 day ago (4 time steps back)

```{r}
modeldata<-AlleStationen
lags <- seq(28)
myLaggedVars<-c("bikes_rented", "bikes_returned")
for(lagVar in myLaggedVars){
  lag_names <- paste(lagVar, formatC(lags, width = nchar(max(lags)), flag = "0"), 
  sep = "_")
  lag_functions <- setNames(paste("dplyr::lag(., ", lags, ")"), lag_names)
  modeldata <- modeldata %>% mutate_at(vars(lagVar), funs_(lag_functions))
}

modeldata<-modeldata %>%
  fill(bikes_rented_01:bikes_rented_28,.direction="up") %>%
  fill(bikes_returned_01:bikes_returned_28,.direction="up")
  
naive_pred <- modeldata %>%
  dplyr::summarise(
    mse_bikes_rented=mse(bikes_rented,bikes_rented_04),
    mse_bikes_returned=mse(bikes_returned,bikes_returned_04),
    n=n()
    
  )
naive_pred
write.csv(modeldata, "Daten/EnrichedData.csv")
```

### Build the input data set for training neural net for a set of stations comprising "hbf_umsteiger" ("Hbf - Main Station Kiel and Umsteiger) based on exploration of data and time-series analysis (lagged variables for 1 week: 4 time steps each day * 7 days = 28 ). The stations are very close and are very frequently used.
### Naive prediction as one base-line model prediction for time_interval t = value of 1 day ago (4 time steps back)

```{r}
modeldata2<-hbf_umsteiger
lags <- seq(28)
myLaggedVars<-c("bikes_rented", "bikes_returned")
for(lagVar in myLaggedVars){
  lag_names <- paste(lagVar, formatC(lags, width = nchar(max(lags)), flag = "0"), 
  sep = "_")
  lag_functions <- setNames(paste("dplyr::lag(., ", lags, ")"), lag_names)
  modeldata2 <- modeldata2 %>% mutate_at(vars(lagVar), funs_(lag_functions))
}

modeldata2<-modeldata2 %>%
  fill(bikes_rented_01:bikes_rented_28,.direction="up") %>%
  fill(bikes_returned_01:bikes_returned_28,.direction="up")
  
naive_pred2 <- modeldata2 %>%
  dplyr::summarise(
    mse_bikes_rented=mse(bikes_rented,bikes_rented_04),
    mse_bikes_returned=mse(bikes_returned,bikes_returned_04),
    n=n()
    
  )

naive_pred2
write.csv(modeldata2, "Daten/Data_HbfUmsteiger.csv")
```



```{r}
```

### Splitting in train and test set for linear modelling of "AllStations" dataset

```{r}
# Set seed to get same result for every run
set.seed(100)
n = nrow(modeldata)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.8, 0.2))

train_dataset_lm = modeldata[split, ]
test_dataset_lm = modeldata[!split, ]

rm(split, n)
```

### Setting up linear regression model as second base-line model for "AllStations"
### variables are not normalized
### Result high adjusted R^2 of 0.75 for a small set of variables chosen
### Very high significance of all variables except christmas

```{r}
model_lm<-lm(bikes_rented ~ bikes_rented_01 + bikes_rented_02 + bikes_rented_03+ bikes_rented_04 + bikes_rented_05 + bikes_returned_01 + as.factor(is_wochenende) +as.factor(is_streiktag)+as.factor(is_weihnachten), train_dataset_lm)
summary(model_lm)
```

### Predicting bikes_rented with linear base-line model, which improves naive prediction.Mean square error reduction from around 1400 for naive prediction to under 1000 for linear model on test-set.

```{r}
pred_train_lm <- predict(model_lm, train_dataset_lm)
pred_test_lm <- predict(model_lm, test_dataset_lm)
mse(train_dataset_lm$bikes_rented, pred_train_lm)
mse(test_dataset_lm$bikes_rented, pred_test_lm)
plot(train_dataset_lm$bikes_rented,pred_train_lm)
plot(test_dataset_lm$bikes_rented,pred_test_lm)
test_dataset_lm$predictionLM <- pred_test_lm
test_dataset_lm <- test_dataset_lm %>%
  mutate(perc_errLM=if_else(bikes_rented!=0,abs(bikes_rented-predictionLM)/bikes_rented,0)) %>%
  mutate(squared_errorLM=(bikes_rented-predictionLM)^2) %>%
  arrange(interval)
train_dataset_lm$predictionLM <- pred_train_lm
train_dataset_lm <- train_dataset_lm %>%
  mutate(perc_errLM=if_else(bikes_rented!=0,abs(bikes_rented-predictionLM)/bikes_rented,0)) %>%
  mutate(squared_errorLM=(bikes_rented-predictionLM)^2) %>%
  arrange(interval)
ggplot(test_dataset_lm) +
  geom_line( aes(x=interval, y=bikes_rented, colour = "Test Data" )) +
  geom_line( aes(x=interval, y=predictionLM, colour = "Prediction" )) +
  scale_colour_manual( values = c("Prediction"="blue", "Test Data"="red") ) +
  labs(title="All stations bikes rented") +
  xlab("time interval") +
  ylab("bikes rented") 
```

### Creating interactive time-series graph for getting an overview of the data

```{r}
don <- xts(x = SF_UmsteigerIV_Summary$n, order.by = SF_UmsteigerIV_Summary$start_rentinterval)
p <- dygraph(don) %>%
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.1, drawGrid = FALSE, colors="#D8AE5A") %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 5, highlightSeriesBackgroundAlpha = 0.2, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1)

# save the widget
library(htmlwidgets)
saveWidget(p, file=paste0( getwd(), "/HtmlWidget/rentsperinterval_umsteiger.html"))
```

### Possibility to write the created datasets to the file system

```{r}
write.csv(SF_clean,"SprotteClean.csv")
write.csv(SF_pivot,"StationDay_Pivot.csv")
write.csv(SF_rents_per_day,"OverallRentsPerDay.csv")
write.csv(SF_Umsteiger,"StationUmsteigerRentReturn.csv")
write.csv(SF_Umsteiger_iv,"StationUmsteigerRentReturnIV.csv")
write.csv(SF_Umsteiger_Summary, "Umsteiger_Summary.csv")
write.csv(SF_UmsteigerIV_Summary, "UmsteigerIV_Summary.csv")
write.csv(SF_Umsteiger_Grouped_AllIntervals, "UmsteigerComplete_Summary.csv")
write.csv(hbf_umsteiger,"HBFUmsteiger_summary.csv")
write.csv(kirchhofallee,"kirchhofallee_summary.csv")
write.csv(AlleStationen,"AlleStationen_summary.csv")
write.csv(AlterMarkt,"AlterMarkt_summary.csv")
write.csv(Westring,"Westring_summary.csv")
write.csv(Wilhelmsplatz,"Wilhelmsplatz_summary.csv")
```

### Only for testing purposes

```{r}
class(SF_clean["time_rental"][[1]][1])
lubi<-as_date(SF_clean["time_rental"][[1]][1])
sometime<-SF_clean["time_rental"][[1]][1]
sometime2<-sometime+hours(10)
someday<-SF_clean["day_rental"][[1]][1]
intstart=hour(sometime)%/%6
startInt<-someday+hours(6*(hour(sometime)%/%6))
startInt
class(startInt)
endInt<-startInt+hours(6)
insome<-lubridate::interval(start=someday+hours(6*(hour(sometime)%/%6)),end=someday+hours(6*(1+hour(sometime)%/%6)), tz="UTC")
insome2<-lubridate::interval(start=someday+hours(6*(hour(sometime2)%/%6)),end=someday+hours(6*(1+hour(sometime2)%/%6)), tz="UTC")
insome@.Data
sometime %within% insome
sometime2 %within% insome2

```



```{r}
DateTimeSeq<-seq(ISOdate(2019,7,2,tz="UTC"), ISOdate(2020,3,31), by = "6 hour") # or "6 hours"
```


```{r}
set.seed(1)
     x.Date <- as.Date(seq(ISOdate(2019,7,2,tz="UTC"), ISOdate(2019,7,13), by = "1 day"))
     #x.Date <- as.Date(paste(2004, rep(1:4, 4:1), sample(1:28, 10), sep = "-"))
     x <- zoo(seq(1,12,1), x.Date)
     
     ## rolling operations for univariate series
     rollmean(x, 3)
     rollmax(x, 3)
     rollmedian(x, 3)
     rollsum(x, 3)
     x
```

